{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Steady_state_pinn",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nK8Kw16cEsS0tDVH8obDDx7YWNYkQw7y",
      "authorship_tag": "ABX9TyNaPy/5J2olCJTRgDvoNaId",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/work/blob/master/Steady_state_pinn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14.0\n"
      ],
      "metadata": {
        "id": "oP9vEw4NA8pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zv011WyVBA4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import time\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from pyDOE import lhs"
      ],
      "metadata": {
        "id": "EhhPf3dABB9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "\n",
        "class pinn:\n",
        "    # Initialize the class\n",
        "    def __init__(self, data_idx, data_t0, data_sup_b_train, layers, N_train, batch_size,load = False, file=None):\n",
        "        \n",
        "#============================================================================================================================#\n",
        "#=============================================Loading data ==================================================================#\n",
        "#============================================================================================================================#\n",
        "        self.data_t0 = data_t0\n",
        "        self.data_sup_b_train = data_sup_b_train\n",
        "        self.data_idx = data_idx\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "        self.lowb = data_idx.min(0)  \n",
        "        self.upb = data_idx.max(0)\n",
        "\n",
        "        self.layers = layers\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=======================================Loading / Initializing NN============================================================#\n",
        "#============================================================================================================================#\n",
        "        if load==True:\n",
        "            # collecting weights and biases from the pickle file\n",
        "            self.weights,self.biases,ac_bc,ac_ic=self.load_parameters(file,self.layers)\n",
        "            \n",
        "            # The adaptive constants are added here so as to start training from a check point\n",
        "            self.beta=0.9\n",
        "            self.adaptive_constant_bcs_val=ac_bc\n",
        "            self.adaptive_constant_ics_val=ac_ic\n",
        "\n",
        "        if load ==False:\n",
        "            # Initializing weights and biases for model using xavier initialization method.\n",
        "            self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "            # Adaptive weighting constants initialized to 1.\n",
        "            self.beta=0.9\n",
        "            self.adaptive_constant_bcs_val=np.array(1.0)  # arw for boundary conditions\n",
        "            self.adaptive_constant_ics_val=np.array(1.0)  # arw for initial conditions\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Specifying Placeholders========================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "\n",
        "        # learning rate placeholder\n",
        "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
        "        \n",
        "        # initial condition placeholders\n",
        "        self.x_ini_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.y_ini_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_ini_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.u_ini_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.v_ini_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "        # boundary condition placeholders\n",
        "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc1_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.t_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.t_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        # domain residual placeholders\n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.y_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "        # defining a placeholder for adaptive constant\n",
        "        self.adaptive_constant_bcs_tf=tf.placeholder(tf.float32,shape=self.adaptive_constant_bcs_val.shape)\n",
        "        self.adaptive_constant_ics_tf=tf.placeholder(tf.float32,shape=self.adaptive_constant_ics_val.shape)\n",
        "        \n",
        "#============================================================================================================================#\n",
        "#=============================================Evaluating Predictions=========================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Initial value predictions\n",
        "        self.u_ini_pred, self.v_ini_pred, self.p_ini_pred = self.net_NS(self.x_ini_tf, self.y_ini_tf, self.t_ini_tf)\n",
        "        \n",
        "        # Boundary value predictions\n",
        "\n",
        "        self.u_bc1_pred, self.v_bc1_pred,_,_,_,_= self.net_f_NS(self.x_bc1_tf, self.y_bc1_tf,self.t_bc1_tf)\n",
        "        self.u_bc2_pred, self.v_bc2_pred,_,_,_,_= self.net_f_NS(self.x_bc2_tf, self.y_bc2_tf,self.t_bc2_tf)\n",
        "        self.u_bc3_pred, self.v_bc3_pred,_,_,_,_= self.net_f_NS(self.x_bc3_tf, self.y_bc3_tf,self.t_bc3_tf)\n",
        "        self.u_bc4_pred, self.v_bc4_pred,_,_,_,_= self.net_f_NS(self.x_bc4_tf, self.y_bc4_tf,self.t_bc4_tf)\n",
        "        \n",
        "        self.U_bc1_pred = tf.concat([self.u_bc1_pred, self.v_bc1_pred], axis=1)\n",
        "        self.U_bc2_pred = tf.concat([self.u_bc2_pred, self.v_bc2_pred], axis=1)\n",
        "        self.U_bc3_pred = tf.concat([self.u_bc3_pred, self.v_bc3_pred], axis=1)\n",
        "        self.U_bc4_pred = tf.concat([self.u_bc4_pred, self.v_bc4_pred], axis=1)\n",
        "\n",
        "        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred, self.f_e_pred =self.net_f_NS(self.x_tf, self.y_tf, self.t_tf)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Defining loss fn===============================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        self.loss_bcs=self.adaptive_constant_bcs_tf*(tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf)+\n",
        "                                                                    tf.square(self.U_bc2_pred - self.U_bc2_tf)+\n",
        "                                                                    tf.square(self.U_bc3_pred - self.U_bc3_tf)+\n",
        "                                                                    tf.square(self.U_bc4_pred - self.U_bc4_tf)))\n",
        "\n",
        "        self.loss_ics=self.adaptive_constant_ics_tf*(tf.reduce_mean(tf.square(self.u_ini_tf - self.u_ini_pred)) + tf.reduce_mean(tf.square(self.v_ini_tf - self.v_ini_pred)))\n",
        "        \n",
        "        # Defining loss function for residual in the domain \n",
        "        \n",
        "        self.loss_res= tf.reduce_mean(tf.square(self.f_u_pred)) + tf.reduce_mean(tf.square(self.f_v_pred)) + tf.reduce_mean(tf.square(self.f_e_pred))\n",
        "\n",
        "\n",
        "        # set loss function\n",
        "        self.loss =self.loss_res + self.loss_bcs + self.loss_ics\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Setting up optimizer===========================================================#\n",
        "#============================================================================================================================#\n",
        "        self.global_step=tf.Variable(0,trainable=False)\n",
        "        starter_learning_rate=1e-3\n",
        "        # Setting up exponential decay of learning rate as training progresses\n",
        "        self.learning_rate=tf.train.exponential_decay(starter_learning_rate,self.global_step,1000,0.9,staircase=False)\n",
        "\n",
        "        # setup lbfgs optimizer\n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
        "                                                                method='L-BFGS-B',\n",
        "                                                                options={'maxiter': 50000,\n",
        "                                                                         'maxfun': 50000,\n",
        "                                                                         'maxcor': 50,\n",
        "                                                                         'maxls': 50,\n",
        "                                                                         'ftol': 1.0 * np.finfo(float).eps})\n",
        "\n",
        "        # setup adam optimizer\n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss,global_step=self.global_step)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Adaptive weighting constants===================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Collecting gradients of the individual loss terms wrt the parameters of the network\n",
        "        self.grad_res=[]\n",
        "        self.grad_bcs=[]\n",
        "        self.grad_ics=[]\n",
        "        \n",
        "        for i in range(len(self.layers)-1):\n",
        "          self.grad_res.append(tf.gradients(self.loss_res,self.weights[i])[0])\n",
        "          self.grad_bcs.append(tf.gradients(self.loss_bcs,self.weights[i])[0])\n",
        "          self.grad_ics.append(tf.gradients(self.loss_ics,self.weights[i])[0])\n",
        "        \n",
        "        # Collecting the adaptive constants for initial and boundary conditions\n",
        "        self.adaptive_constant_bcs_list=[]\n",
        "        self.adaptive_constant_ics_list=[]\n",
        "\n",
        "        self.adaptive_constant_bcs_log=[]\n",
        "        self.adaptive_constant_ics_log=[]\n",
        "\n",
        "        for i in range(len(self.layers)-1):\n",
        "          self.adaptive_constant_bcs_list.append(\n",
        "              tf.reduce_max(tf.abs(self.grad_res[i])) / tf.reduce_mean(tf.abs(self.grad_bcs[i]))\n",
        "          )\n",
        "          self.adaptive_constant_ics_list.append(\n",
        "              tf.reduce_max(tf.abs(self.grad_res[i])) / tf.reduce_mean(tf.abs(self.grad_ics[i]))\n",
        "          )\n",
        "        \n",
        "        self.adaptive_constant_bcs=tf.reduce_max(tf.stack(self.adaptive_constant_bcs_list))\n",
        "        self.adpative_constant_ics=tf.reduce_max(tf.stack(self.adaptive_constant_ics_list))\n",
        "        \n",
        "#============================================================================================================================#\n",
        "#=============================================initializing session===========================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Initializing Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Data in batches================================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Creating batches for domain data\n",
        "    def summon_batch_domain(self,batch_size,start):\n",
        "      ''' \n",
        "      This function returns a list containing randomnly picked points from the domain\n",
        "      '''\n",
        "      data=copy.deepcopy(self.data_idx)\n",
        "      points=[]\n",
        "\n",
        "      for i in range(start,len(data),batch_size): # Something is wrong here\n",
        "\n",
        "        points.append(data[i:i+batch_size,0:3])\n",
        "        break # breaks after the first iteration\n",
        "\n",
        "      return points\n",
        "\n",
        "    def domain_batches(self,batch_size):\n",
        "      '''\n",
        "      This function is used to call the domain data points in batches\n",
        "      This function returns an array of arrays of shape(438,1) each of the 438 arrays\n",
        "      will have batch size number of points\n",
        "      '''\n",
        "      batches=[]\n",
        "      num_batches=math.ceil(self.N_train/batch_size)\n",
        "\n",
        "      for i in range(0,len(data_idx),batch_size):\n",
        "        batches.append(self.summon_batch_domain(batch_size,i))\n",
        "\n",
        "      return np.asarray(batches,dtype=object) \n",
        "\n",
        "# Creating batches for initial conditions\n",
        "    def summon_batch_ic(self,batch_size,start):\n",
        "\n",
        "      data=copy.deepcopy(data_t0)\n",
        "      points=[]\n",
        "\n",
        "      for i in range(start,len(data),batch_size): \n",
        "\n",
        "        points.append(data[i:i+batch_size])\n",
        "        break # breaks after the first iteration\n",
        "\n",
        "      return points\n",
        "    def ic_batches(self,batch_size):\n",
        "      batches=[]\n",
        "      num_batches=math.ceil(self.data_t0.shape[0]/batch_size)\n",
        "\n",
        "      for i in range(0,len(self.data_t0),batch_size):\n",
        "        batches.append(self.summon_batch_ic(batch_size,i))\n",
        "\n",
        "      return np.asarray(batches,dtype=object)       \n",
        "\n",
        "# Creatng batches for boundary condition\n",
        "    def summon_batch_bc(self,batch_size,bc_con,start):\n",
        "      \n",
        "      if bc_con==1:\n",
        "        data=copy.deepcopy(data_sup_b_train[0:20000,:])\n",
        "      elif bc_con==2:\n",
        "        data=copy.deepcopy(data_sup_b_train[20000:40000,:])\n",
        "      elif bc_con==3:\n",
        "        data=copy.deepcopy(data_sup_b_train[40000:60000,:])\n",
        "      elif bc_con==4:\n",
        "        data=copy.deepcopy(data_sup_b_train[60000:80000,:])\n",
        "\n",
        "      points=[]\n",
        "\n",
        "      for i in range(start,len(data),batch_size): \n",
        "\n",
        "        points.append(data[i:i+batch_size])\n",
        "        break # breaks after the first iteration\n",
        "\n",
        "      return points\t\n",
        "    \n",
        "    def bc_batches(self,batch_size,bc_con):\n",
        "\n",
        "      batches=[]\n",
        "      \n",
        "      for i in range(0,int(len(data_sup_b_train)/4),batch_size):\n",
        "        batches.append(self.summon_batch_bc(batch_size,bc_con,i))\n",
        "\n",
        "      return np.asarray(batches,dtype=object) \n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Utility fns= ==================================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "# initialize the weights and biases\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "# xavier used to initialize the weight\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "#===========================================================================================================#\n",
        "#====================================Transfer Learning======================================================#\n",
        "#===========================================================================================================#\n",
        "    def save_parameters(self,fileDr):\n",
        "        weights=self.sess.run(self.weights)\n",
        "        biases= self.sess.run(self.biases)\n",
        "        \n",
        "        ac_bc=(self.adaptive_constant_bcs_val)\n",
        "        ac_ic=(self.adaptive_constant_ics_val)\n",
        "                \n",
        "        root_path=Path(\"D:\\Vamsi\\Python scripts\")\n",
        "        my_path=root_path/fileDr\n",
        "        \n",
        "        with open(my_path,'wb') as f:\n",
        "            pickle.dump([weights,biases,ac_bc,ac_ic],f)\n",
        "            print(\"Parameters are saved in pickle file\")\n",
        "\n",
        "    def load_parameters(self,fileDr,layers):\n",
        "        tf_weights=[]\n",
        "        tf_biases=[]\n",
        "        num_layers=len(layers)\n",
        "        \n",
        "        root_path=Path(\"D:\\Vamsi\\Python scripts\")\n",
        "        my_path=root_path/fileDr\n",
        "        \n",
        "        # returns the weights and biases of the network as np array\n",
        "        with open(my_path,'rb') as f:\n",
        "            weights,biases,ac_bc,ac_ic=pickle.load(f)\n",
        "            assert num_layers == (len(weights)+1)\n",
        "        \n",
        "        # returns the weights and biases of the network as tf.variable\n",
        "        for num in range(0,num_layers-1):\n",
        "            W=tf.Variable(weights[num])\n",
        "            b=tf.Variable(biases[num])\n",
        "            tf_weights.append(W)\n",
        "            tf_biases.append(b)\n",
        "         \n",
        "        \n",
        "        print(\"Parameters are loaded succesffuly\")\n",
        "        \n",
        "        return tf_weights,tf_biases,ac_bc,ac_ic\n",
        "    \n",
        "    \n",
        "#============================================================================================================================#\n",
        "#=============================================Neural_net setup===============================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "\n",
        "    def neural_net(self, X):\n",
        "        num_layers = len(self.weights) + 1\n",
        "\n",
        "        H = 2.0 * (X - self.lowb) / (self.upb - self.lowb) - 1.0\n",
        "\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = self.weights[-1]\n",
        "        b = self.biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "\n",
        "    # supervised data driven\n",
        "    def net_NS(self, x, y, t):\n",
        "\n",
        "        \n",
        "        psi_p = self.neural_net(tf.concat([x, y, t], 1))\n",
        "        psi = psi_p[:, 0:1]\n",
        "        p   = psi_p[:, 1:2]\n",
        "        \n",
        "        u=tf.gradients(psi,y)[0]\n",
        "        v=(-1) * tf.gradients(psi,x)[0]\n",
        "\n",
        "        return u, v, p\n",
        "\n",
        "    # unsupervised NS residual\n",
        "    def net_f_NS(self, x, y, t):\n",
        "\n",
        "        psi_p = self.neural_net(tf.concat([x, y, t], 1))\n",
        "        psi = psi_p[:, 0:1]\n",
        "        p   = psi_p[:, 1:2]\n",
        "        \n",
        "        u=tf.gradients(psi,y)[0]\n",
        "        v=(-1) * tf.gradients(psi,x)[0]\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_y = tf.gradients(u, y)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        u_yy = tf.gradients(u_y, y)[0]\n",
        "\n",
        "        v_t = tf.gradients(v, t)[0]\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "        v_y = tf.gradients(v, y)[0]\n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "        v_yy = tf.gradients(v_y, y)[0]\n",
        "\n",
        "        p_x = tf.gradients(p, x)[0]\n",
        "        p_y = tf.gradients(p, y)[0]\n",
        "\n",
        "        f_u = u_t + (u * u_x + v * u_y) + p_x - 0.01 * (u_xx + u_yy)\n",
        "        f_v = v_t + (u * v_x + v * v_y) + p_y - 0.01 * (v_xx + v_yy)\n",
        "        f_e = 0.0\n",
        "\n",
        "        return u, v, p, f_u, f_v, f_e\n",
        "\n",
        "    def callback(self, loss):\n",
        "        print('Loss: %.3e' % loss)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Training algorithms============================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "\n",
        "    def Adam_train(self, epochs=5000,file_save='',file_path=''):\n",
        "\n",
        "        start_time = time.time()\n",
        "        tf_dict = {\n",
        "              self.x_ini_tf: self.data_t0[:,0:1], # Initial values\n",
        "              self.y_ini_tf: self.data_t0[:,1:2],\n",
        "              self.t_ini_tf: self.data_t0[:,2:3], \n",
        "              self.u_ini_tf: self.data_t0[:,3:4],\n",
        "              self.v_ini_tf: self.data_t0[:,4:5], \n",
        "\n",
        "              self.x_bc1_tf: self.data_sup_b_train[0:20000,0:1] ,# boundary conditions\n",
        "              self.y_bc1_tf: self.data_sup_b_train[0:20000,1:2], \n",
        "              self.t_bc1_tf: self.data_sup_b_train[0:20000,2:3],\n",
        "              self.U_bc1_tf: self.data_sup_b_train[0:20000,3:5],\n",
        "\n",
        "              self.x_bc2_tf: self.data_sup_b_train[20000:40000,0:1] ,\n",
        "              self.y_bc2_tf: self.data_sup_b_train[20000:40000,1:2] , \n",
        "              self.t_bc2_tf: self.data_sup_b_train[20000:40000,2:3] ,\n",
        "              self.U_bc2_tf: self.data_sup_b_train[20000:40000,3:5] ,\n",
        "\n",
        "              self.x_bc3_tf: self.data_sup_b_train[40000:60000,0:1] ,\n",
        "              self.y_bc3_tf: self.data_sup_b_train[40000:60000,1:2] , \n",
        "              self.t_bc3_tf: self.data_sup_b_train[40000:60000,2:3] ,\n",
        "              self.U_bc3_tf: self.data_sup_b_train[40000:60000,3:5] ,\n",
        "\n",
        "              self.x_bc4_tf: self.data_sup_b_train[60000:80000,0:1] ,\n",
        "              self.y_bc4_tf: self.data_sup_b_train[60000:80000,1:2] , \n",
        "              self.t_bc4_tf: self.data_sup_b_train[60000:80000,2:3] ,\n",
        "              self.U_bc4_tf: self.data_sup_b_train[60000:80000,3:5] ,\n",
        "\n",
        "              self.x_tf: self.data_idx[:,0:1] , # domain \n",
        "              self.y_tf: self.data_idx[:,1:2] , \n",
        "              self.t_tf: self.data_idx[:,2:3] ,\n",
        "\n",
        "              self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
        "              self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val}\n",
        "        \n",
        "        for epoch in (range(epochs)):\n",
        "\n",
        "            # add the tf dict here \n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if epoch % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                adaptive_constant_bcs_val=self.sess.run(self.adaptive_constant_bcs,tf_dict)\n",
        "                adaptive_constant_ics_val=self.sess.run(self.adpative_constant_ics,tf_dict)\n",
        "\n",
        "                self.adaptive_constant_bcs_val=adaptive_constant_bcs_val* (1.0-self.beta)+self.beta*self.adaptive_constant_bcs_val\n",
        "                self.adaptive_constant_ics_val=adaptive_constant_ics_val* (1.0-self.beta)+self.beta*self.adaptive_constant_ics_val\n",
        "\n",
        "                print('epoch: %d, Loss: %.3e, Time: %.2f' %\n",
        "                      (epoch, loss_value, elapsed))\n",
        "                print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
        "                print(\"constant_ics_val: {:.3f}\".format(self.adaptive_constant_ics_val))\n",
        "                start_time = time.time()\n",
        "\n",
        "                fil=open(file_path,\"w\")\n",
        "                fil.writelines(\"epoch: \"+str(it)+\" \"+\"\\n\"+\"loss: \"+str(loss_value)+\"\\n\"+\" elapsed: \"+str(elapsed))\n",
        "                fil.flush()\n",
        "            \n",
        "             if epoch !=0 and epoch %1000==0:\n",
        "                self.save_parameters(file_save)\n",
        "\n",
        "# two step train BFGS used to finetune the result\n",
        "    def BFGS_train(self):\n",
        "\n",
        "        tf_dict = {self.x_ini_tf: self.x0, self.y_ini_tf: self.y0,\n",
        "                   self.t_ini_tf: self.t0, self.u_ini_tf: self.u0,\n",
        "                   self.v_ini_tf: self.v0, self.x_boundary_tf: self.xb,\n",
        "                   self.y_boundary_tf: self.yb, self.t_boundary_tf: self.tb,\n",
        "                   self.u_boundary_tf: self.ub, self.v_boundary_tf: self.vb,\n",
        "                   self.x_tf: self.x, self.y_tf: self.y, self.t_tf: self.t}\n",
        "\n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict=tf_dict,\n",
        "                                fetches=[self.loss],\n",
        "                                loss_callback=self.callback)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Predicting for test_data=======================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "\n",
        "    def predict(self, x_star, y_star, t_star):\n",
        "\n",
        "        tf_dict = {self.x_tf: x_star, self.y_tf: y_star, self.t_tf: t_star}\n",
        "\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
        "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
        "\n",
        "        return u_star, v_star, p_star"
      ],
      "metadata": {
        "id": "ILmKomsGBDU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dom_data(N_train,T,min_max):\n",
        "    xmax,xmin,ymax,ymin,tmax,tmin=min_max\n",
        "\n",
        "    num_pts=int(N_train/T) \n",
        "\n",
        "    # Using latin hypercube sampling\n",
        "    lb=np.array(([xmin,ymin,tmin]))\n",
        "    ub=np.array(([xmax,ymax,tmax]))\n",
        "    \n",
        "    data_idx=lb+(ub-lb)*lhs(3,N_train)\n",
        "\n",
        "    j=0\n",
        "    for i in range(0,len(data_idx),num_pts):\n",
        "        data_idx[i:i+num_pts,2:3]=j\n",
        "        i=i+1\n",
        "        j=j+0.1\n",
        "\n",
        "    return data_idx\n",
        "\n",
        "def load_data(N_train,num_bound_pts,flag=False):\n",
        "\n",
        " #====================== Loading data from csv files=========================#\n",
        "\tpath=r\"C:\\Users\\vamsi_oe20s302\\Downloads\\all_pressures\"\n",
        "\n",
        "\tuvel=pd.read_csv(path+r\"/u_vel.csv\")\n",
        "\tuvel=uvel.to_numpy()\n",
        "\n",
        "\tvvel=pd.read_csv(path+r\"/v_vel.csv\")\n",
        "\tvvel=vvel.to_numpy()\n",
        "\n",
        "\tpress=pd.read_csv(path+r\"/static_press.csv\")\n",
        "\tpress=press.to_numpy()\n",
        "\n",
        "\txy=pd.read_csv(path+r\"/xy.csv\")\n",
        "\txy=xy.to_numpy()\n",
        "\n",
        "\tt=pd.read_csv(path+r\"/time.csv\")\n",
        "\tt=t.to_numpy()\n",
        "\n",
        "\tN=xy.shape[0]\n",
        "\tT=t.shape[0]\n",
        "\tidx1 = np.arange(0, N*T)\n",
        "\n",
        "\tXX = np.tile(xy[:,0:1], (1,T)) # N x T\n",
        "\tYY = np.tile(xy[:,1:2], (1,T)) # N x T\n",
        "\tTT = np.tile(t, (1,N)).T # N x T\n",
        "\n",
        "\tx = XX.flatten()[:,None] # NT x 1\n",
        "\ty = YY.flatten()[:,None] # NT x 1\n",
        "\tt = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "\tu = uvel.flatten()[:,None] # NT x 1\n",
        "\tv = vvel.flatten()[:,None] # NT x 1\n",
        "\tp = press.flatten()[:,None] # NT x 1\n",
        "\n",
        "\n",
        "\tax=np.unique(x)\n",
        "\txmin=min(ax)\n",
        "\txmax=max(ax)\n",
        "\tay=np.unique(y)\n",
        "\tymin=min(ay)\n",
        "\tymax=max(ay)\n",
        "\tat=np.unique(t)\n",
        "\ttmin=min(at)\n",
        "\ttmax=max(at)\n",
        "\n",
        "\n",
        "\n",
        "\tdata1=np.concatenate([x ,y ,t , u , v ,p ],1)\n",
        "\n",
        "\t#======================== domain =================================#\n",
        "\tdata2=data1[:,:][data1[:,2]<=20] # Taking data upto time step 20.0\n",
        "\t \n",
        "\tdata3=data2[:,:][data2[:,0]>=xmin] # Taking data greater than xmin\n",
        "\n",
        "\tdata4=data3[:,:][data3[:,0]<=xmax] # Taking data less than xmax\n",
        "\n",
        "\tdata5=data4[:,:][data4[:,1]>=ymin] # Taking data greater than ymin\n",
        "\n",
        "\tdata_domain=data5[:,:][data5[:,1]<=ymax] # Taking data less than ymax\n",
        "  \n",
        "\tmin_max=[xmax,xmin,ymax,ymin,tmax,tmin]\n",
        "\tdata_idx=dom_data(N_train,T,min_max)\n",
        " \n",
        "\t#======================= initial ==================================#\n",
        "\tdata_t0=data_domain[:,:][data_domain[:,2]==0]\n",
        "\t#===================== boundary ======================================#\n",
        "\tbc1_data=data_domain[:,:][data_domain[:,1]==ymax]\n",
        "\tbc2_data=data_domain[:,:][data_domain[:,0]==xmin]\n",
        "\tbc3_data=data_domain[:,:][data_domain[:,0]==xmax]\n",
        "\tbc4_data=data_domain[:,:][data_domain[:,1]==ymin]\n",
        "\n",
        "\t# If you want to train the model on all boundary points flag full----> True\n",
        "\tif flag == True:\n",
        "\t\tdata_sup_b_train = [bc1_data,bc2_data,bc3_data,bc4_data]\n",
        "\t\n",
        "\t# If you want to train the model on randomly selected points across time flag ful----> False\n",
        "\telse:\n",
        "\t\tbidx = np.random.choice(bc1_data.shape[0], num_bound_pts, replace=True)\n",
        "\n",
        "\t\t# Randomly selecting points on boundary conditions from 20000\n",
        "\t\tbc1=bc1_data[bidx]\n",
        "\t\tbc2=bc2_data[bidx]\n",
        "\t\tbc3=bc3_data[bidx]\n",
        "\t\tbc4=bc4_data[bidx]\n",
        "\n",
        "\t\tdata_sup_b_train=[bc1,bc2,bc3,bc4]\n",
        "\n",
        "\treturn data_idx,data_t0,data_sup_b_train"
      ],
      "metadata": {
        "id": "fusKjKI2BFIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    N_train = 14000\n",
        "    N_bound = 200\n",
        "    batch_size= 32\n",
        "\n",
        "    layers = [3, 50,50,50, 2]\n",
        "    #layers=[3]+10*[4*50]+[2]\n",
        "    \n",
        "    # loading data\n",
        "    data_idx,data_t0,data_sup_b_train = load_data(N_train,N_bound,)\n",
        "    \n",
        "    # Initializing the model for training\n",
        "    model = pinn(data_idx,data_t0,data_sup_b_train,layers,N_train,batch_size,load=False,file=None)\n",
        "    \n",
        "#     Initializing model for testing by loading the pickle file\n",
        "#     filedr='' # Enter the name of the pickle file\n",
        "#     model= pinn(data_idx,data_t0,data_sup_b_train,layers,N_train,batch_size,load=True,file=filedr)\n",
        "    \n",
        "#     Enter the name of the parameters file\n",
        "    file_save='params_pr' \n",
        "    file_path=r'C:\\Users\\vamsi_oe20s302\\Desktop\\tf_test.txt'\n",
        "\n",
        "#      Training the model\n",
        "    model.Adam_train(200000,file_save,file_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AmQ_Fgg-BKx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_testdata(snap):\n",
        "    path=r\"/content/drive/MyDrive/all_pressures\"\n",
        "\n",
        "    uvel=pd.read_csv(path+r\"/u_vel.csv\")\n",
        "    uvel=uvel.to_numpy()\n",
        "\n",
        "    vvel=pd.read_csv(path+r\"/v_vel.csv\")\n",
        "    vvel=vvel.to_numpy()\n",
        "\n",
        "    press=pd.read_csv(path+r\"/static_press.csv\")\n",
        "    press=press.to_numpy()\n",
        "    \n",
        "    xy=pd.read_csv(path+r\"/xy.csv\")\n",
        "    xy=xy.to_numpy()\n",
        "    \n",
        "    t=pd.read_csv(path+r\"/time.csv\")\n",
        "    t=t.to_numpy()\n",
        "    \n",
        "    N=xy.shape[0]\n",
        "    T=t.shape[0]\n",
        "    \n",
        "    TT = np.tile(t, (1,N)).T # N x T\n",
        "    \n",
        "    x_star = xy[:, 0:1]\n",
        "    y_star = xy[:, 1:2]\n",
        "    t_star = TT[:, snap]\n",
        "    X_star=[x_star,y_star,t_star]\n",
        "    \n",
        "    u_star = uvel[:, snap]\n",
        "    v_star = vvel[:, snap]\n",
        "    p_star = press[:, snap]\n",
        "    Y_star=[u_star,v_star,p_star]\n",
        "    \n",
        "    return X_star,Y_star"
      ],
      "metadata": {
        "id": "f7DWRGSNBM1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing the load functionality for errors :\n",
        "# filedr='params_3_laptop_run' # Enter the name of the pickle file\n",
        "# model= pinn(data_idx,data_t0,data_sup_b_train,layers,N_train,batch_size,load=True,file=filedr)\n",
        "\n",
        "\n",
        "# # Prediction\n",
        "X_star,Y_star=load_testdata(np.array([100]))\n",
        "u_pred, v_pred, p_pred = model.predict(X_star[0],X_star[1],X_star[2])\n",
        "\n",
        "# # Error\n",
        "error_u = np.linalg.norm(Y_star[0] - u_pred, 2) / np.linalg.norm(Y_star[0], 2)\n",
        "error_v = np.linalg.norm(Y_star[1] - v_pred, 2) / np.linalg.norm(Y_star[1], 2)\n",
        "error_p = np.linalg.norm(Y_star[2] - p_pred, 2) / np.linalg.norm(Y_star[2], 2)\n",
        "\n",
        "print('Error u: %e' % error_u)\n",
        "print('Error v: %e' % error_v)\n",
        "print('Error p: %e' % error_p)"
      ],
      "metadata": {
        "id": "nBRohF1FBO0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uplot=np.reshape(u_pred,(100,100))\n",
        "plt.imshow(uplot)\n",
        "plt.title(\"Predicted u_velocity\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "QxZPpHQkBQMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upl=np.reshape(Y_star[0],(100,100))\n",
        "plt.imshow(upl)\n",
        "plt.title(\"Actual u_velocity\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "LurPAu3wBTP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}