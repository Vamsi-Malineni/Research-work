{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Steady_state_pinn",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/Vamsi-Malineni/work/blob/master/Steady_state_pinn.ipynb",
      "authorship_tag": "ABX9TyO9wxIb/2pHFZc54F9AVCke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/work/blob/master/Steady_state_pinn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.0\n"
      ],
      "metadata": {
        "id": "oP9vEw4NA8pP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c011419-598a-4ead-e601-85bc573d3ae3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 67.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.46.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=44882d62bde9b4c06c7d45fe890eec76ae7e3a807db6e9fd7aca1b65f80f7399\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zv011WyVBA4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyDOE"
      ],
      "metadata": {
        "id": "rnppH0ZgQ7KQ",
        "outputId": "90539c88-bdb3-4805-8866-b21b10ad1c36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.4.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=f94244e530235f7102760697771f47ba2403ad1a247050234fe06b7903d27884\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ce/8a/87b25c685bfeca1872d13b8dc101e087a9c6e3fb5ebb47022a\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import time\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from pyDOE import lhs"
      ],
      "metadata": {
        "id": "EhhPf3dABB9G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "\n",
        "class pinn:\n",
        "    # Initialize the class\n",
        "    #model = pinn(domain_data,boundary_data,layers,N_train,batch_size,load=False,file=None)\n",
        "    def __init__(self, domain_data, boundary_data, layers, N_train, batch_size,load = False, file=None):\n",
        "        \n",
        "#============================================================================================================================#\n",
        "#=============================================Loading data ==================================================================#\n",
        "#============================================================================================================================#\n",
        "        self.data_sup_b_train = boundary_data\n",
        "        self.data_idx = domain_data\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "        # Check why this is here \n",
        "        self.lowb = self.data_idx.min(0)  \n",
        "        self.upb = self.data_idx.max(0)\n",
        "\n",
        "        self.layers = layers\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=======================================Loading / Initializing NN============================================================#\n",
        "#============================================================================================================================#\n",
        "        if load==True:\n",
        "            # collecting weights and biases from the pickle file\n",
        "            self.weights,self.biases,ac_bc=self.load_parameters(file,self.layers)\n",
        "            \n",
        "            # The adaptive constants are added here so as to start training from a check point\n",
        "            self.beta=0.9\n",
        "            self.adaptive_constant_bcs_val=ac_bc\n",
        "\n",
        "        if load ==False:\n",
        "            # Initializing weights and biases for model using xavier initialization method.\n",
        "            self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "            # Adaptive weighting constants initialized to 1.\n",
        "            self.beta=0.9\n",
        "            self.adaptive_constant_bcs_val=np.array(1.0)  # arw for boundary conditions\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Specifying Placeholders========================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "\n",
        "        # learning rate placeholder\n",
        "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
        "        \n",
        "       \n",
        "        # boundary condition placeholders\n",
        "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc1_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "\n",
        "        # domain residual placeholders\n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.y_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "        # defining a placeholder for adaptive constant\n",
        "        self.adaptive_constant_bcs_tf=tf.placeholder(tf.float32,shape=self.adaptive_constant_bcs_val.shape)\n",
        "        \n",
        "#============================================================================================================================#\n",
        "#=============================================Evaluating Predictions=========================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        \n",
        "        # Boundary value predictions\n",
        "\n",
        "        self.u_bc1_pred, self.v_bc1_pred,_,_,_,_= self.net_f_NS(self.x_bc1_tf, self.y_bc1_tf)\n",
        "        self.u_bc2_pred, self.v_bc2_pred,_,_,_,_= self.net_f_NS(self.x_bc2_tf, self.y_bc2_tf)\n",
        "        self.u_bc3_pred, self.v_bc3_pred,_,_,_,_= self.net_f_NS(self.x_bc3_tf, self.y_bc3_tf)\n",
        "        self.u_bc4_pred, self.v_bc4_pred,_,_,_,_= self.net_f_NS(self.x_bc4_tf, self.y_bc4_tf)\n",
        "        \n",
        "        self.U_bc1_pred = tf.concat([self.u_bc1_pred, self.v_bc1_pred], axis=1)\n",
        "        self.U_bc2_pred = tf.concat([self.u_bc2_pred, self.v_bc2_pred], axis=1)\n",
        "        self.U_bc3_pred = tf.concat([self.u_bc3_pred, self.v_bc3_pred], axis=1)\n",
        "        self.U_bc4_pred = tf.concat([self.u_bc4_pred, self.v_bc4_pred], axis=1)\n",
        "\n",
        "        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred, self.f_e_pred =self.net_f_NS(self.x_tf, self.y_tf)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Defining loss fn===============================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        self.loss_bcs=self.adaptive_constant_bcs_tf*(tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf)+\n",
        "                                                                    tf.square(self.U_bc2_pred - self.U_bc2_tf)+\n",
        "                                                                    tf.square(self.U_bc3_pred - self.U_bc3_tf)+\n",
        "                                                                    tf.square(self.U_bc4_pred - self.U_bc4_tf)))\n",
        "\n",
        "        \n",
        "        # Defining loss function for residual in the domain \n",
        "        \n",
        "        self.loss_res= tf.reduce_mean(tf.square(self.f_u_pred)) + tf.reduce_mean(tf.square(self.f_v_pred)) + tf.reduce_mean(tf.square(self.f_e_pred))\n",
        "\n",
        "\n",
        "        # set loss function\n",
        "        self.loss =self.loss_res + self.loss_bcs\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Setting up optimizer===========================================================#\n",
        "#============================================================================================================================#\n",
        "        self.global_step=tf.Variable(0,trainable=False)\n",
        "        starter_learning_rate=1e-3\n",
        "        # Setting up exponential decay of learning rate as training progresses\n",
        "        self.learning_rate=tf.train.exponential_decay(starter_learning_rate,self.global_step,1000,0.9,staircase=False)\n",
        "\n",
        "        # setup lbfgs optimizer\n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
        "                                                                method='L-BFGS-B',\n",
        "                                                                options={'maxiter': 50000,\n",
        "                                                                         'maxfun': 50000,\n",
        "                                                                         'maxcor': 50,\n",
        "                                                                         'maxls': 50,\n",
        "                                                                         'ftol': 1.0 * np.finfo(float).eps})\n",
        "\n",
        "        # setup adam optimizer\n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss,global_step=self.global_step)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Adaptive weighting constants===================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Collecting gradients of the individual loss terms wrt the parameters of the network\n",
        "        self.grad_res=[]\n",
        "        self.grad_bcs=[]\n",
        "        \n",
        "        for i in range(len(self.layers)-1):\n",
        "          self.grad_res.append(tf.gradients(self.loss_res,self.weights[i])[0])\n",
        "          self.grad_bcs.append(tf.gradients(self.loss_bcs,self.weights[i])[0])\n",
        "        \n",
        "        # Collecting the adaptive constants for initial and boundary conditions\n",
        "        self.adaptive_constant_bcs_list=[]\n",
        "\n",
        "        self.adaptive_constant_bcs_log=[]\n",
        "\n",
        "        for i in range(len(self.layers)-1):\n",
        "          self.adaptive_constant_bcs_list.append(\n",
        "              tf.reduce_max(tf.abs(self.grad_res[i])) / tf.reduce_mean(tf.abs(self.grad_bcs[i]))\n",
        "          )\n",
        "        \n",
        "        self.adaptive_constant_bcs=tf.reduce_max(tf.stack(self.adaptive_constant_bcs_list))\n",
        "        \n",
        "#============================================================================================================================#\n",
        "#=============================================initializing session===========================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Initializing Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Data in batches================================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "        # Creating batches for domain data\n",
        "    def summon_batch_domain(self,batch_size,start):\n",
        "      ''' \n",
        "      This function returns a list containing randomnly picked points from the domain\n",
        "      '''\n",
        "      data=copy.deepcopy(self.data_idx)\n",
        "      points=[]\n",
        "\n",
        "      for i in range(start,len(data),batch_size): # Something is wrong here\n",
        "\n",
        "        points.append(data[i:i+batch_size,0:3])\n",
        "        break # breaks after the first iteration\n",
        "\n",
        "      return points\n",
        "\n",
        "    def domain_batches(self,batch_size):\n",
        "      '''\n",
        "      This function is used to call the domain data points in batches\n",
        "      This function returns an array of arrays of shape(438,1) each of the 438 arrays\n",
        "      will have batch size number of points\n",
        "      '''\n",
        "      batches=[]\n",
        "      num_batches=math.ceil(self.N_train/batch_size)\n",
        "\n",
        "      for i in range(0,len(data_idx),batch_size):\n",
        "        batches.append(self.summon_batch_domain(batch_size,i))\n",
        "\n",
        "      return np.asarray(batches,dtype=object) \n",
        "\n",
        "# Creating batches for initial conditions\n",
        "    def summon_batch_ic(self,batch_size,start):\n",
        "\n",
        "      data=copy.deepcopy(data_t0)\n",
        "      points=[]\n",
        "\n",
        "      for i in range(start,len(data),batch_size): \n",
        "\n",
        "        points.append(data[i:i+batch_size])\n",
        "        break # breaks after the first iteration\n",
        "\n",
        "      return points\n",
        "    def ic_batches(self,batch_size):\n",
        "      batches=[]\n",
        "      num_batches=math.ceil(self.data_t0.shape[0]/batch_size)\n",
        "\n",
        "      for i in range(0,len(self.data_t0),batch_size):\n",
        "        batches.append(self.summon_batch_ic(batch_size,i))\n",
        "\n",
        "      return np.asarray(batches,dtype=object)       \n",
        "\n",
        "# Creatng batches for boundary condition\n",
        "    def summon_batch_bc(self,batch_size,bc_con,start):\n",
        "      \n",
        "      if bc_con==1:\n",
        "        data=copy.deepcopy(data_sup_b_train[0:20000,:])\n",
        "      elif bc_con==2:\n",
        "        data=copy.deepcopy(data_sup_b_train[20000:40000,:])\n",
        "      elif bc_con==3:\n",
        "        data=copy.deepcopy(data_sup_b_train[40000:60000,:])\n",
        "      elif bc_con==4:\n",
        "        data=copy.deepcopy(data_sup_b_train[60000:80000,:])\n",
        "\n",
        "      points=[]\n",
        "\n",
        "      for i in range(start,len(data),batch_size): \n",
        "\n",
        "        points.append(data[i:i+batch_size])\n",
        "        break # breaks after the first iteration\n",
        "\n",
        "      return points\t\n",
        "    \n",
        "    def bc_batches(self,batch_size,bc_con):\n",
        "\n",
        "      batches=[]\n",
        "      \n",
        "      for i in range(0,int(len(data_sup_b_train)/4),batch_size):\n",
        "        batches.append(self.summon_batch_bc(batch_size,bc_con,i))\n",
        "\n",
        "      return np.asarray(batches,dtype=object) \n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Utility fns= ==================================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "# initialize the weights and biases\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "# xavier used to initialize the weight\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "#===========================================================================================================#\n",
        "#====================================Transfer Learning======================================================#\n",
        "#===========================================================================================================#\n",
        "    def save_parameters(self,fileDr):\n",
        "        weights=self.sess.run(self.weights)\n",
        "        biases= self.sess.run(self.biases)\n",
        "        \n",
        "        ac_bc=(self.adaptive_constant_bcs_val)\n",
        "                \n",
        "        root_path=Path(\"D:\\Vamsi\\Python scripts\")\n",
        "        my_path=root_path/fileDr\n",
        "        \n",
        "        with open(my_path,'wb') as f:\n",
        "            pickle.dump([weights,biases,ac_bc],f)\n",
        "            print(\"Parameters are saved in pickle file\")\n",
        "\n",
        "    def load_parameters(self,fileDr,layers):\n",
        "        tf_weights=[]\n",
        "        tf_biases=[]\n",
        "        num_layers=len(layers)\n",
        "        \n",
        "        root_path=Path(\"D:\\Vamsi\\Python scripts\")\n",
        "        my_path=root_path/fileDr\n",
        "        \n",
        "        # returns the weights and biases of the network as np array\n",
        "        with open(my_path,'rb') as f:\n",
        "            weights,biases,ac_bc=pickle.load(f)\n",
        "            assert num_layers == (len(weights)+1)\n",
        "        \n",
        "        # returns the weights and biases of the network as tf.variable\n",
        "        for num in range(0,num_layers-1):\n",
        "            W=tf.Variable(weights[num])\n",
        "            b=tf.Variable(biases[num])\n",
        "            tf_weights.append(W)\n",
        "            tf_biases.append(b)\n",
        "         \n",
        "        \n",
        "        print(\"Parameters are loaded succesffuly\")\n",
        "        \n",
        "        return tf_weights,tf_biases,ac_bc\n",
        "    \n",
        "    \n",
        "#============================================================================================================================#\n",
        "#=============================================Neural_net setup===============================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "\n",
        "    def neural_net(self, X):\n",
        "        num_layers = len(self.weights) + 1\n",
        "\n",
        "        H = 2.0 * (X - self.lowb) / (self.upb - self.lowb) - 1.0\n",
        "\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = self.weights[-1]\n",
        "        b = self.biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "\n",
        "    # supervised data driven\n",
        "    def net_NS(self, x, y):\n",
        "\n",
        "        \n",
        "        psi_p = self.neural_net(tf.concat([x, y], 1))\n",
        "        psi = psi_p[:, 0:1]\n",
        "        p   = psi_p[:, 1:2]\n",
        "        \n",
        "        u=tf.gradients(psi,y)[0]\n",
        "        v=(-1) * tf.gradients(psi,x)[0]\n",
        "\n",
        "        return u, v, p\n",
        "\n",
        "    # unsupervised NS residual\n",
        "    def net_f_NS(self, x, y):\n",
        "\n",
        "        psi_p = self.neural_net(tf.concat([x, y], 1))\n",
        "        psi = psi_p[:, 0:1]\n",
        "        p   = psi_p[:, 1:2]\n",
        "        \n",
        "        u=tf.gradients(psi,y)[0]\n",
        "        v=(-1) * tf.gradients(psi,x)[0]\n",
        "        \n",
        "        \n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_y = tf.gradients(u, y)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        u_yy = tf.gradients(u_y, y)[0]\n",
        "\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "        v_y = tf.gradients(v, y)[0]\n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "        v_yy = tf.gradients(v_y, y)[0]\n",
        "\n",
        "        p_x = tf.gradients(p, x)[0]\n",
        "        p_y = tf.gradients(p, y)[0]\n",
        "\n",
        "        f_u = (u * u_x + v * u_y) + p_x - 0.01 * (u_xx + u_yy)\n",
        "        f_v = (u * v_x + v * v_y) + p_y - 0.01 * (v_xx + v_yy)\n",
        "        f_e = 0.0\n",
        "\n",
        "        return u, v, p, f_u, f_v, f_e\n",
        "\n",
        "    def callback(self, loss):\n",
        "        print('Loss: %.3e' % loss)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Training algorithms============================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "\n",
        "    def Adam_train(self, epochs=5000,file_save='',file_path=''):\n",
        "\n",
        "        start_time = time.time()\n",
        "        tf_dict = {\n",
        "\n",
        "              self.x_bc1_tf: self.data_sup_b_train[0][:,0:1] ,# boundary conditions\n",
        "              self.y_bc1_tf: self.data_sup_b_train[0][:,1:2], \n",
        "              self.U_bc1_tf: self.data_sup_b_train[0][:,3:5],\n",
        "\n",
        "              self.x_bc2_tf: self.data_sup_b_train[1][:,0:1] ,\n",
        "              self.y_bc2_tf: self.data_sup_b_train[1][:,1:2] , \n",
        "              self.U_bc2_tf: self.data_sup_b_train[1][:,3:5] ,\n",
        "\n",
        "              self.x_bc3_tf: self.data_sup_b_train[2][:,0:1] ,\n",
        "              self.y_bc3_tf: self.data_sup_b_train[2][:,1:2] , \n",
        "              self.U_bc3_tf: self.data_sup_b_train[2][:,3:5] ,\n",
        "\n",
        "              self.x_bc4_tf: self.data_sup_b_train[3][:,0:1] ,\n",
        "              self.y_bc4_tf: self.data_sup_b_train[3][:,1:2] , \n",
        "              self.U_bc4_tf: self.data_sup_b_train[3][:,3:5] ,\n",
        "\n",
        "              self.x_tf: self.data_idx[:,0:1] , # domain \n",
        "              self.y_tf: self.data_idx[:,1:2] , \n",
        "\n",
        "              self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val}\n",
        "\n",
        "        for epoch in (range(epochs)):\n",
        "\n",
        "            # add the tf dict here \n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if epoch % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                adaptive_constant_bcs_val=self.sess.run(self.adaptive_constant_bcs,tf_dict)\n",
        "\n",
        "                self.adaptive_constant_bcs_val=adaptive_constant_bcs_val* (1.0-self.beta)+self.beta*self.adaptive_constant_bcs_val\n",
        "\n",
        "                print('epoch: %d, Loss: %.3e, Time: %.2f' %\n",
        "                      (epoch, loss_value, elapsed))\n",
        "                print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
        "                start_time = time.time()\n",
        "\n",
        "                # fil=open(file_path,\"w\")\n",
        "                # fil.writelines(\"epoch: \"+str(it)+\" \"+\"\\n\"+\"loss: \"+str(loss_value)+\"\\n\"+\" elapsed: \"+str(elapsed))\n",
        "                # fil.flush()\n",
        "            \n",
        "            if epoch !=0 and epoch %1000==0:\n",
        "               self.save_parameters(file_save)\n",
        "\n",
        "# two step train BFGS used to finetune the result\n",
        "    def BFGS_train(self):\n",
        "\n",
        "        tf_dict = {self.x_ini_tf: self.x0, self.y_ini_tf: self.y0,\n",
        "                   self.t_ini_tf: self.t0, self.u_ini_tf: self.u0,\n",
        "                   self.v_ini_tf: self.v0, self.x_boundary_tf: self.xb,\n",
        "                   self.y_boundary_tf: self.yb, self.t_boundary_tf: self.tb,\n",
        "                   self.u_boundary_tf: self.ub, self.v_boundary_tf: self.vb,\n",
        "                   self.x_tf: self.x, self.y_tf: self.y, self.t_tf: self.t}\n",
        "\n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict=tf_dict,\n",
        "                                fetches=[self.loss],\n",
        "                                loss_callback=self.callback)\n",
        "\n",
        "#============================================================================================================================#\n",
        "#=============================================Predicting for test_data=======================================================#\n",
        "#============================================================================================================================#\n",
        "\n",
        "\n",
        "    def predict(self, x_star, y_star, t_star):\n",
        "\n",
        "        tf_dict = {self.x_tf: x_star, self.y_tf: y_star}\n",
        "\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
        "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
        "\n",
        "        return u_star, v_star, p_star"
      ],
      "metadata": {
        "id": "ILmKomsGBDU_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data and plotting data\n"
      ],
      "metadata": {
        "id": "5VG_lJXCRh07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dom_data(N_train,min_max):\n",
        "    xmax,xmin,ymax,ymin=min_max\n",
        "    lb=np.array(([xmin,ymin]))\n",
        "    ub=np.array(([xmax,ymax]))\n",
        "\n",
        "    colloc_points=lb+(ub-lb)*lhs(2,N_train)\n",
        "\n",
        "    return colloc_points\n",
        "\n",
        "def load_data(N_train,num_bound_pts,flag=False):\n",
        "    path=r\"/content/drive/MyDrive/steady_state_data\"\n",
        "\n",
        "    uvel=pd.read_csv(path+r\"/u_vel.csv\")\n",
        "    uvel=uvel.to_numpy()\n",
        "\n",
        "    vvel=pd.read_csv(path+r\"/v_vel.csv\")\n",
        "    vvel=vvel.to_numpy()\n",
        "\n",
        "    press=pd.read_csv(path+r\"/press.csv\")\n",
        "    press=press.to_numpy()\n",
        "\n",
        "    xy=pd.read_csv(path+r\"/xy.csv\")\n",
        "    xy=xy.to_numpy()\n",
        "\n",
        "    # need add unsupervised part\n",
        "    x=xy[:,0:1]\n",
        "    y=xy[:,1:2]\n",
        "\n",
        "    ax=np.unique(x)\n",
        "    xmin=min(ax)\n",
        "    xmax=max(ax)\n",
        "\n",
        "    ay=np.unique(y)\n",
        "    ymin=min(ay)\n",
        "    ymax=max(ay)\n",
        "\n",
        "\n",
        "    # # Concatenating all the points\n",
        "    data_domain=np.concatenate([x ,y , uvel , vvel ,press ],1)\n",
        "    # # Defining boundary conditions for the data\n",
        "    bc1_data=data_domain[:,:][data_domain[:,1]==ymax]\n",
        "    bc2_data=data_domain[:,:][data_domain[:,0]==xmin]\n",
        "    bc3_data=data_domain[:,:][data_domain[:,0]==xmax]\n",
        "    bc4_data=data_domain[:,:][data_domain[:,1]==ymin]\n",
        "\n",
        "    # # Creating boundary conditions dataset size=(80000,6)\n",
        "\n",
        "    if flag == True:\n",
        "            data_sup_b_train = [bc1_data,bc2_data,bc3_data,bc4_data]\n",
        "        \n",
        "    # If you want to train the model on randomly selected points across time flag ful----> False\n",
        "    else:\n",
        "        bidx = np.random.choice(bc1_data.shape[0], num_bound_pts, replace=True)\n",
        "\n",
        "        # Randomly selecting points on boundary conditions from 20000\n",
        "        bc1=bc1_data[bidx]\n",
        "        bc2=bc2_data[bidx]\n",
        "        bc3=bc3_data[bidx]\n",
        "        bc4=bc4_data[bidx]\n",
        "\n",
        "        data_sup_b_train=[bc1,bc2,bc3,bc4]\n",
        "    \n",
        "    min_max=[xmax,xmin,ymax,ymin]\n",
        "    colloc_points =dom_data(N_train,min_max)\n",
        "    \n",
        "    return colloc_points,data_sup_b_train\n",
        "\n",
        "\n",
        "def scatter(domain_data,boundary_data):\n",
        "    x=domain_data[:,0:1]\n",
        "    y=domain_data[:,1:2]\n",
        "\n",
        "    b1=boundary_data[0]\n",
        "    b2=boundary_data[1]\n",
        "    b3=boundary_data[2]\n",
        "    b4=boundary_data[3]\n",
        "\n",
        "    xb1=b1[:,0:1]\n",
        "    xb2=b2[:,0:1]\n",
        "    xb3=b3[:,0:1]\n",
        "    xb4=b4[:,0:1]\n",
        "\n",
        "    yb1=b1[:,1:2]\n",
        "    yb2=b2[:,1:2]\n",
        "    yb3=b3[:,1:2]\n",
        "    yb4=b4[:,1:2]\n",
        "\n",
        "    bc1=plt.scatter(xb1,yb1,c='blue',s=10)\n",
        "    bc2=plt.scatter(xb2,yb2,c='green',s=10)\n",
        "    bc3=plt.scatter(xb3,yb3,c='black',s=10)\n",
        "    bc4=plt.scatter(xb4,yb4,c='yellow',s=10)\n",
        "    dom=plt.scatter(x,y,c='red',s=1)\n",
        "\n",
        "    plt.legend((dom,bc1,bc2,bc3,bc4),\n",
        "    ('domain','bc1','bc2','bc3','bc4'),loc='lower left',\n",
        "    ncol=2,fontsize=8)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fusKjKI2BFIO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 140000\n",
        "N_bound = 200\n",
        "    \n",
        "dd,bd=load_data(N_train,N_bound,False)\n",
        "scatter(dd,bd)"
      ],
      "metadata": {
        "id": "Ry3zxYGkREYj",
        "outputId": "9381a346-7e42-4b74-ef28-e08e4a72dd23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running PINN"
      ],
      "metadata": {
        "id": "TvVWQv0TRnn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    N_train = 140000\n",
        "    N_bound = 200\n",
        "    batch_size= 32\n",
        "\n",
        "    layers = [2, 50,50,50, 2]\n",
        "    #layers=[3]+10*[4*50]+[2]\n",
        "    \n",
        "    # loading data\n",
        "    domain_data,boundary_data=load_data(N_train,N_bound,False)\n",
        "    \n",
        "    # Initializing the model for training\n",
        "    model = pinn(domain_data,boundary_data,layers,N_train,batch_size,load=False,file=None)\n",
        "    \n",
        "#     Initializing model for testing by loading the pickle file\n",
        "#     filedr='' # Enter the name of the pickle file\n",
        "#     model= pinn(data_idx,data_t0,data_sup_b_train,layers,N_train,batch_size,load=True,file=filedr)\n",
        "    \n",
        "#     Enter the name of the parameters file\n",
        "    file_save='params_pr' \n",
        "    file_path=r'C:\\Users\\vamsi_oe20s302\\Desktop\\tf_test.txt'\n",
        "\n",
        "#      Training the model\n",
        "    model.Adam_train(200000,file_save,file_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AmQ_Fgg-BKx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0200f502-e880-4c06-b3b5-0421a2a938d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "epoch: 0, Loss: 4.270e-01, Time: 6.17\n",
            "constant_bcs_val: 1.914\n",
            "epoch: 10, Loss: 1.214e-01, Time: 4.48\n",
            "constant_bcs_val: 3.308\n",
            "epoch: 20, Loss: 8.985e-02, Time: 4.49\n",
            "constant_bcs_val: 7.356\n",
            "epoch: 30, Loss: 6.604e-02, Time: 4.50\n",
            "constant_bcs_val: 7.963\n",
            "epoch: 40, Loss: 5.456e-02, Time: 4.51\n",
            "constant_bcs_val: 8.209\n",
            "epoch: 50, Loss: 4.812e-02, Time: 4.56\n",
            "constant_bcs_val: 9.132\n",
            "epoch: 60, Loss: 4.483e-02, Time: 4.58\n",
            "constant_bcs_val: 11.283\n",
            "epoch: 70, Loss: 4.289e-02, Time: 4.53\n",
            "constant_bcs_val: 12.343\n",
            "epoch: 80, Loss: 4.142e-02, Time: 4.54\n",
            "constant_bcs_val: 12.552\n",
            "epoch: 90, Loss: 4.024e-02, Time: 4.55\n",
            "constant_bcs_val: 12.257\n",
            "epoch: 100, Loss: 3.930e-02, Time: 4.56\n",
            "constant_bcs_val: 11.782\n",
            "epoch: 110, Loss: 3.852e-02, Time: 4.57\n",
            "constant_bcs_val: 11.129\n",
            "epoch: 120, Loss: 3.787e-02, Time: 4.58\n",
            "constant_bcs_val: 10.671\n",
            "epoch: 130, Loss: 3.731e-02, Time: 4.56\n",
            "constant_bcs_val: 10.283\n",
            "epoch: 140, Loss: 3.683e-02, Time: 4.57\n",
            "constant_bcs_val: 9.928\n",
            "epoch: 150, Loss: 3.643e-02, Time: 4.55\n",
            "constant_bcs_val: 9.596\n",
            "epoch: 160, Loss: 3.611e-02, Time: 4.55\n",
            "constant_bcs_val: 9.341\n",
            "epoch: 170, Loss: 3.586e-02, Time: 4.55\n",
            "constant_bcs_val: 9.115\n",
            "epoch: 180, Loss: 3.567e-02, Time: 4.54\n",
            "constant_bcs_val: 8.937\n",
            "epoch: 190, Loss: 3.552e-02, Time: 4.53\n",
            "constant_bcs_val: 8.807\n",
            "epoch: 200, Loss: 3.540e-02, Time: 4.55\n",
            "constant_bcs_val: 8.703\n",
            "epoch: 210, Loss: 3.530e-02, Time: 4.55\n",
            "constant_bcs_val: 8.624\n",
            "epoch: 220, Loss: 3.521e-02, Time: 4.54\n",
            "constant_bcs_val: 8.579\n",
            "epoch: 230, Loss: 3.513e-02, Time: 4.55\n",
            "constant_bcs_val: 8.566\n",
            "epoch: 240, Loss: 3.504e-02, Time: 4.55\n",
            "constant_bcs_val: 8.576\n",
            "epoch: 250, Loss: 3.496e-02, Time: 4.63\n",
            "constant_bcs_val: 8.600\n",
            "epoch: 260, Loss: 3.487e-02, Time: 4.56\n",
            "constant_bcs_val: 8.633\n",
            "epoch: 270, Loss: 3.478e-02, Time: 4.55\n",
            "constant_bcs_val: 8.671\n",
            "epoch: 280, Loss: 3.468e-02, Time: 4.56\n",
            "constant_bcs_val: 8.713\n",
            "epoch: 290, Loss: 3.458e-02, Time: 4.56\n",
            "constant_bcs_val: 8.760\n",
            "epoch: 300, Loss: 3.447e-02, Time: 4.56\n",
            "constant_bcs_val: 8.811\n",
            "epoch: 310, Loss: 3.436e-02, Time: 4.55\n",
            "constant_bcs_val: 8.869\n",
            "epoch: 320, Loss: 3.424e-02, Time: 4.56\n",
            "constant_bcs_val: 8.933\n",
            "epoch: 330, Loss: 3.411e-02, Time: 4.55\n",
            "constant_bcs_val: 9.004\n",
            "epoch: 340, Loss: 3.398e-02, Time: 4.55\n",
            "constant_bcs_val: 9.084\n",
            "epoch: 350, Loss: 3.384e-02, Time: 4.55\n",
            "constant_bcs_val: 9.174\n",
            "epoch: 360, Loss: 3.369e-02, Time: 4.55\n",
            "constant_bcs_val: 9.273\n",
            "epoch: 370, Loss: 3.352e-02, Time: 4.55\n",
            "constant_bcs_val: 9.381\n",
            "epoch: 380, Loss: 3.335e-02, Time: 4.54\n",
            "constant_bcs_val: 9.499\n",
            "epoch: 390, Loss: 3.317e-02, Time: 4.55\n",
            "constant_bcs_val: 9.626\n",
            "epoch: 400, Loss: 3.298e-02, Time: 4.55\n",
            "constant_bcs_val: 9.763\n",
            "epoch: 410, Loss: 3.278e-02, Time: 4.56\n",
            "constant_bcs_val: 9.908\n",
            "epoch: 420, Loss: 3.256e-02, Time: 4.55\n",
            "constant_bcs_val: 10.061\n",
            "epoch: 430, Loss: 3.234e-02, Time: 4.56\n",
            "constant_bcs_val: 10.219\n",
            "epoch: 440, Loss: 3.211e-02, Time: 4.56\n",
            "constant_bcs_val: 10.379\n",
            "epoch: 450, Loss: 3.187e-02, Time: 4.55\n",
            "constant_bcs_val: 10.539\n",
            "epoch: 460, Loss: 3.163e-02, Time: 4.55\n",
            "constant_bcs_val: 10.697\n",
            "epoch: 470, Loss: 3.139e-02, Time: 4.55\n",
            "constant_bcs_val: 10.847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing model"
      ],
      "metadata": {
        "id": "v2OSJU1pRtHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_testdata(snap):\n",
        "    path=r\"/content/drive/MyDrive/all_pressures\"\n",
        "\n",
        "    uvel=pd.read_csv(path+r\"/u_vel.csv\")\n",
        "    uvel=uvel.to_numpy()\n",
        "\n",
        "    vvel=pd.read_csv(path+r\"/v_vel.csv\")\n",
        "    vvel=vvel.to_numpy()\n",
        "\n",
        "    press=pd.read_csv(path+r\"/static_press.csv\")\n",
        "    press=press.to_numpy()\n",
        "    \n",
        "    xy=pd.read_csv(path+r\"/xy.csv\")\n",
        "    xy=xy.to_numpy()\n",
        "    \n",
        "    t=pd.read_csv(path+r\"/time.csv\")\n",
        "    t=t.to_numpy()\n",
        "    \n",
        "    N=xy.shape[0]\n",
        "    T=t.shape[0]\n",
        "    \n",
        "    TT = np.tile(t, (1,N)).T # N x T\n",
        "    \n",
        "    x_star = xy[:, 0:1]\n",
        "    y_star = xy[:, 1:2]\n",
        "    t_star = TT[:, snap]\n",
        "    X_star=[x_star,y_star,t_star]\n",
        "    \n",
        "    u_star = uvel[:, snap]\n",
        "    v_star = vvel[:, snap]\n",
        "    p_star = press[:, snap]\n",
        "    Y_star=[u_star,v_star,p_star]\n",
        "    \n",
        "    return X_star,Y_star"
      ],
      "metadata": {
        "id": "f7DWRGSNBM1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing the load functionality for errors :\n",
        "# filedr='params_3_laptop_run' # Enter the name of the pickle file\n",
        "# model= pinn(data_idx,data_t0,data_sup_b_train,layers,N_train,batch_size,load=True,file=filedr)\n",
        "\n",
        "\n",
        "# # Prediction\n",
        "X_star,Y_star=load_testdata(np.array([100]))\n",
        "u_pred, v_pred, p_pred = model.predict(X_star[0],X_star[1],X_star[2])\n",
        "\n",
        "# # Error\n",
        "error_u = np.linalg.norm(Y_star[0] - u_pred, 2) / np.linalg.norm(Y_star[0], 2)\n",
        "error_v = np.linalg.norm(Y_star[1] - v_pred, 2) / np.linalg.norm(Y_star[1], 2)\n",
        "error_p = np.linalg.norm(Y_star[2] - p_pred, 2) / np.linalg.norm(Y_star[2], 2)\n",
        "\n",
        "print('Error u: %e' % error_u)\n",
        "print('Error v: %e' % error_v)\n",
        "print('Error p: %e' % error_p)"
      ],
      "metadata": {
        "id": "nBRohF1FBO0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uplot=np.reshape(u_pred,(100,100))\n",
        "plt.imshow(uplot)\n",
        "plt.title(\"Predicted u_velocity\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "QxZPpHQkBQMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upl=np.reshape(Y_star[0],(100,100))\n",
        "plt.imshow(upl)\n",
        "plt.title(\"Actual u_velocity\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "LurPAu3wBTP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}