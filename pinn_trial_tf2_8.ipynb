{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pinn_trial_tf2.8",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb",
      "authorship_tag": "ABX9TyMCqaBd9JoXypYsM/6W/f7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WOp0QV73geKH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp \n",
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import time \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pinn:\n",
        "  def __init__(self,layers,optimizer,ub,lb):\n",
        "    \n",
        "    self.model=tf.keras.Sequential()\n",
        "    \n",
        "    self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    \n",
        "    self.model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X-lb)/(ub-lb)-1))\n",
        "    # Here X should be tf.concat([x_train,y_train,t_train],1) such that scaling \n",
        "    # happens properly\n",
        "\n",
        "    for width in layers[1:]:\n",
        "      self.model.add(tf.keras.layers.Dense(\n",
        "          width,activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'\n",
        "      ))\n",
        "\n",
        "  # finding the sizes of weights and biases for each layer in the neural network  \n",
        "    self.sizes_w=[]\n",
        "    self.sizes_b=[]\n",
        "\n",
        "    for i,width in enumerate(layers):\n",
        "      if i!=1:\n",
        "        self.sizes_w.append(int(width*layers[1]))\n",
        "        self.sizes_b.append(int(width if i!=0 else layers[1]))\n",
        "    \n",
        "    self.dtype=tf.float32\n",
        "\n",
        "  # Defining two variables that have to be learnt (lambda1,lambda2)\n",
        "    self.lambda1=tf.Variable([0.0],dtype=self.dtype)\n",
        "    self.lambda2=tf.Variable([0.0],dtype=self.dtype)\n",
        "\n",
        "    self.optimizer=optimizer\n",
        "\n",
        "  def get_parameters(self,numpy=False):\n",
        "    l1=self.lambda1\n",
        "    l2=self.lambda2\n",
        "    if numpy :\n",
        "      return l1.numpy()[0],l2.numpy()[0]\n",
        "    return l1,l2\n",
        "\n",
        "  def f_g_model(self,xtrain,ytrain,ttrain):\n",
        "    l1,l2=self.get_parameters()\n",
        "    # Inputs to be watched by gradient tape\n",
        "    x_f=tf.convert_to_tensor(xtrain,dtype=self.dtype)\n",
        "    y_f=tf.convert_to_tensor(ytrain,dtype=self.dtype)\n",
        "    t_f=tf.convert_to_tensor(ttrain,dtype=self.dtype)\n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      g.watch(x_f)\n",
        "      g.watch(y_f)\n",
        "      g.watch(t_f)\n",
        "      # Stacking the input variables and sending them to model\n",
        "      xf=tf.stack([x_f[:,0],y_f[:,0],t_f[:,0]],axis=1)\n",
        "\n",
        "      psi_and_p=self.model(xf)\n",
        "\n",
        "      psi=psi_and_p[:,0:1]\n",
        "      p=psi_and_p[:,1:2]\n",
        "\n",
        "      u= g.gradient(psi,y_f)\n",
        "      v=-g.gradient(psi,x_f)\n",
        "\n",
        "      ut=g.gradient(u,t_f)\n",
        "      ux=g.gradient(u,x_f)\n",
        "      uy=g.gradient(u,y_f)\n",
        "      \n",
        "      vt=g.gradient(v,t_f)\n",
        "      vx=g.gradient(v,x_f)\n",
        "      vy=g.gradient(v,y_f)\n",
        "\n",
        "      px=g.gradient(p,x_f)\n",
        "      py=g.gradient(p,y_f)\n",
        "    \n",
        "    uxx=g.gradient(ux,x_f)\n",
        "    uyy=g.gradient(uy,y_f)\n",
        "    vxx=g.gradient(vx,x_f)\n",
        "    vyy=g.gradient(vy,y_f)\n",
        "\n",
        "    del g\n",
        "    f = ut+l1*(u*ux+v*uy)+px-l2*(uxx+uyy) \n",
        "    g = vt+l1*(u*vx+v*vy)+py-l2*(vxx+vyy)\n",
        "    \n",
        "    return u,v,p,f,g\n",
        "    \n",
        "    def loss(self,x,y,t,u,v):\n",
        "      u_pred,v_pred,p_pred,f_pred,g_pred=self.f_g_model(x,y,t)\n",
        "      \n",
        "      loss_eq=tf.reduce_mean(tf.square(u-u_pred))+ \\\n",
        "              tf.reduce_mean(tf.square(v-v_pred))+ \\\n",
        "              tf.reduce_mean(tf.square(f_pred))+ \\\n",
        "              tf.reduce_mean(tf.square(g_pred))\n",
        "      return loss_eq\n",
        "    \n",
        "    def training_variables(self):\n",
        "      variables=self.model.trainable_variables\n",
        "      variables.extend([self.lambda1,self.lambda2])\n",
        "      return variables\n",
        "\n",
        "    def grad(self,x,y,t,u,v):\n",
        "      with tf.GradientTape() as g:\n",
        "        loss_value=self.loss(x,y,t,u,v)\n",
        "      return loss_value,g.gradient(loss_value,self.training_variables())\n",
        "    \n",
        "    def get_weights(self):\n",
        "      weights_and_biases=[]\n",
        "      \n",
        "      for layer in self.model.layers[1:]:\n",
        "        wandb=layer.get_weights()\n",
        "        w=wandb[0].flatten()\n",
        "        b=wandb[1]\n",
        "        weights_and_biases.extend(w)\n",
        "        weights_and_biases.extend(b)\n",
        "      weights_and_biases(self.lambda1.numpy())\n",
        "      weights_and_biases(self.lambda2.numpy())\n",
        "\n",
        "      return tf.convert_to_tensor(weights_and_biases,dtype=self.dtype)\n",
        "\n",
        "    def set_weights(self,w):\n",
        "      \n"
      ],
      "metadata": {
        "id": "xpmFYlk1Llzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepdata(N_train):\n",
        "    data = scipy.io.loadmat('/content/drive/MyDrive/cylinder_nektar_wake.mat')\n",
        "           \n",
        "    U_star = data['U_star'] # N x 2 x T\n",
        "    t_star = data['t'] # T x 1\n",
        "    X_star = data['X_star'] # N x 2\n",
        "    \n",
        "    N = X_star.shape[0]\n",
        "    T = t_star.shape[0]\n",
        "    \n",
        "    # Rearrange Data \n",
        "    XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "    YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "    TT = np.tile(t_star, (1,N)).T # N x T\n",
        "    \n",
        "    UU = U_star[:,0,:] # N x T\n",
        "    VV = U_star[:,1,:] # N x T\n",
        "    \n",
        "    x = XX.flatten()[:,None] # NT x 1\n",
        "    y = YY.flatten()[:,None] # NT x 1\n",
        "    t = TT.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    u = UU.flatten()[:,None] # NT x 1\n",
        "    v = VV.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    # Training Data    \n",
        "    idx = np.random.choice(N*T, N_train, replace=False)\n",
        "    x_train = x[idx,:]\n",
        "    y_train = y[idx,:]\n",
        "    t_train = t[idx,:]\n",
        "    u_train = u[idx,:]\n",
        "    v_train = v[idx,:]\n",
        "    \n",
        "    lb=tf.math.reduce_min(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0) \n",
        "    ub=tf.math.reduce_max(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0)\n",
        "        \n",
        "\n",
        "    return x_train,y_train,t_train,u_train,v_train,lb,ub\n"
      ],
      "metadata": {
        "id": "KTtmMeF2P19e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.stack([x[:,0],y[:,0],t[:,0]],axis=1))"
      ],
      "metadata": {
        "id": "78e2EIhubmEy",
        "outputId": "61c78038-b58a-468f-805b-ec7bd74acd0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 1.42424242 -1.18367347  6.5       ]\n",
            " [ 6.37373737  0.85714286 10.5       ]\n",
            " [ 1.56565657  1.75510204  2.4       ]\n",
            " ...\n",
            " [ 6.44444444  1.51020408 17.1       ]\n",
            " [ 2.2020202   0.69387755  5.4       ]\n",
            " [ 2.13131313  0.93877551 15.2       ]], shape=(5000, 3), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.concat([x,y,t],1)[0])"
      ],
      "metadata": {
        "id": "QnFSO8QgbpO1",
        "outputId": "2052852d-04e1-42d1-859f-d50445d6a574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 1.42424242 -1.18367347  6.5       ], shape=(3,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ucKZbytWnfUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}