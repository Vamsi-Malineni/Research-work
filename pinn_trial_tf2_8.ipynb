{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pinn_trial_tf2.8",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14DnT3HYppuSIEU2Enl7vd1VgaRxJRj9X",
      "authorship_tag": "ABX9TyO8/7vpCzmOCQDxiI81onlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "6SBbs0be8YNW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepdata(N_train):\n",
        "    #data = scipy.io.loadmat('Downloads/cylinder_nektar_wake.mat')\n",
        "    data=scipy.io.loadmat('/content/drive/MyDrive/cylinder_nektar_wake.mat')\n",
        "    U_star = data['U_star'] # N x 2 x T\n",
        "    t_star = data['t'] # T x 1\n",
        "    X_star = data['X_star'] # N x 2\n",
        "    \n",
        "    N = X_star.shape[0]\n",
        "    T = t_star.shape[0]\n",
        "    \n",
        "    # Rearrange Data \n",
        "    XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "    YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "    TT = np.tile(t_star, (1,N)).T # N x T\n",
        "    \n",
        "    UU = U_star[:,0,:] # N x T\n",
        "    VV = U_star[:,1,:] # N x T\n",
        "    \n",
        "    x = XX.flatten()[:,None] # NT x 1\n",
        "    y = YY.flatten()[:,None] # NT x 1\n",
        "    t = TT.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    u = UU.flatten()[:,None] # NT x 1\n",
        "    v = VV.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    # Training Data    \n",
        "    idx = np.random.choice(N*T, N_train, replace=False)\n",
        "    x_train = x[idx,:]\n",
        "    y_train = y[idx,:]\n",
        "    t_train = t[idx,:]\n",
        "    u_train = u[idx,:]\n",
        "    v_train = v[idx,:]\n",
        "    \n",
        "    lb=tf.math.reduce_min(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0) \n",
        "    ub=tf.math.reduce_max(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0)\n",
        "    lb=tf.cast(lb,dtype=tf.float32)\n",
        "    ub=tf.cast(ub,dtype=tf.float32)\n",
        "\n",
        "    return x_train,y_train,t_train,u_train,v_train,lb,ub\n"
      ],
      "metadata": {
        "id": "jJsXQ9pZ9RvN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "  def __init__(self, frequency=10):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    self.model = model\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
        "    print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)"
      ],
      "metadata": {
        "id": "V0f0m-TcF3Sm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LBFGS\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "metadata": {
        "cellView": "code",
        "id": "lDYEWyOdG6kO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HYPER PARAMETERS\n",
        "\n",
        "# Data size on the solution u\n",
        "N_u = 2000\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.001)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "IBgt-C5vG8M1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, ub, lb):\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Defining the two additional trainable variables for identification\n",
        "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
        "    \n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  # The actual PINN\n",
        "  def __f_model(self, xtrain,ytrain,ttrain):\n",
        "    l1, l2 = self.get_params()\n",
        "    # Separating the collocation coordinates\n",
        "    x_f=tf.convert_to_tensor(xtrain,dtype=self.dtype)\n",
        "    y_f=tf.convert_to_tensor(ytrain,dtype=self.dtype)\n",
        "    t_f=tf.convert_to_tensor(ttrain,dtype=self.dtype)\n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      g.watch(x_f)\n",
        "      g.watch(y_f)\n",
        "      g.watch(t_f)\n",
        "      # Stacking the input variables and sending them to model\n",
        "      xf=tf.stack([x_f[:,0],y_f[:,0],t_f[:,0]],axis=1)\n",
        "\n",
        "      psi_and_p=self.u_model(xf)\n",
        "\n",
        "      psi=psi_and_p[:,0:1]\n",
        "      p=psi_and_p[:,1:2]\n",
        "\n",
        "      u= g.gradient(psi,y_f)\n",
        "      v=-g.gradient(psi,x_f)\n",
        "\n",
        "      ut=g.gradient(u,t_f)\n",
        "      ux=g.gradient(u,x_f)\n",
        "      uy=g.gradient(u,y_f)\n",
        "      \n",
        "      vt=g.gradient(v,t_f)\n",
        "      vx=g.gradient(v,x_f)\n",
        "      vy=g.gradient(v,y_f)\n",
        "\n",
        "      px=g.gradient(p,x_f)\n",
        "      py=g.gradient(p,y_f)\n",
        "    \n",
        "    uxx=g.gradient(ux,x_f)\n",
        "    uyy=g.gradient(uy,y_f)\n",
        "    vxx=g.gradient(vx,x_f)\n",
        "    vyy=g.gradient(vy,y_f)\n",
        "\n",
        "    del g\n",
        "    f = ut+l1*(u*ux+v*uy)+px-l2*(uxx+uyy) \n",
        "    g = vt+l1*(u*vx+v*vy)+py-l2*(vxx+vyy)\n",
        "    \n",
        "    return u,v,p,f,g\n",
        "  # Defining custom loss\n",
        "  def __loss(self, x,y,t,u,v):\n",
        "    u_pred,v_pred,p_pred,f_pred,g_pred=self.__f_model(x,y,t)\n",
        "      \n",
        "    loss_eq=tf.reduce_mean(tf.square(u-u_pred))+ \\\n",
        "            tf.reduce_mean(tf.square(v-v_pred))+ \\\n",
        "            tf.reduce_mean(tf.square(f_pred))+ \\\n",
        "            tf.reduce_mean(tf.square(g_pred))\n",
        "    return loss_eq\n",
        "\n",
        "  def __grad(self, x,y,t,u,v):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(x,y,t,u,v)\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "      w = []\n",
        "      for layer in self.u_model.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "      w.extend(self.lambda_1.numpy())\n",
        "      w.extend(self.lambda_2.numpy())\n",
        "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    if numpy:\n",
        "      return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self,x,y,t,u,v, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    x=tf.convert_to_tensor(x,dtype=self.dtype)\n",
        "    y=tf.convert_to_tensor(y,dtype=self.dtype)\n",
        "    t=tf.convert_to_tensor(t,dtype=self.dtype)\n",
        "    u=tf.convert_to_tensor(u,dtype=self.dtype)\n",
        "    v=tf.convert_to_tensor(v,dtype=self.dtype)\n",
        "      \n",
        "\n",
        "    def log_train_epoch(epoch, loss, is_iter):\n",
        "      l1, l2 = self.get_params(numpy=True)\n",
        "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
        "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(x,y,t,u,v)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      log_train_epoch(epoch, loss_value, False)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "   \n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        tape.watch(self.lambda_1)\n",
        "        tape.watch(self.lambda_2)\n",
        "        loss_value = self.__loss(x,y,t,u,v)\n",
        "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "   \n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True, log_train_epoch)\n",
        "    \n",
        "    l1, l2 = self.get_params(numpy=True)\n",
        "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.__f_model(X_star)\n",
        "    return u_star.numpy(), f_star.numpy()"
      ],
      "metadata": {
        "id": "UZ79k-ZlG-Al"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,t,u,v,lb,ub=prepdata(N_u)\n",
        "layers=[3,20,20,20,20,20,20,20,20,2]\n",
        "lambdas_star=[1,0.01]\n",
        "\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(x,y,t,u,v,tf_epochs, nt_config)\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
        "print(\"l1: \", lambda_1_pred)\n",
        "print(\"l2: \", lambda_2_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW4L83yiMe01",
        "outputId": "827d7cde-0814-4c35-911b-c38eeb45406e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.0\n",
            "Eager execution: True\n",
            "GPU-accerelated: False\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_3 (Lambda)           (None, 3)                 0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 20)                80        \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 2)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,062\n",
            "Trainable params: 3,062\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:01  loss = 9.3501e-01  error = 8.7625e-01  l1 = -0.000371  l2 = 0.002479\n",
            "tf_epoch =     10  elapsed = 00:07  loss = 5.7884e-01  error = 8.7673e-01  l1 = -0.002990  l2 = 0.002495\n",
            "tf_epoch =     20  elapsed = 00:13  loss = 5.0985e-01  error = 8.8045e-01  l1 = -0.012304  l2 = 0.002514\n",
            "tf_epoch =     30  elapsed = 00:20  loss = 4.8391e-01  error = 8.8565e-01  l1 = -0.023841  l2 = 0.002525\n",
            "tf_epoch =     40  elapsed = 00:26  loss = 4.6848e-01  error = 8.8868e-01  l1 = -0.030456  l2 = 0.002531\n",
            "tf_epoch =     50  elapsed = 00:32  loss = 4.5543e-01  error = 8.8925e-01  l1 = -0.032259  l2 = 0.002538\n",
            "tf_epoch =     60  elapsed = 00:39  loss = 4.4337e-01  error = 8.8695e-01  l1 = -0.029009  l2 = 0.002551\n",
            "tf_epoch =     70  elapsed = 00:45  loss = 4.2911e-01  error = 8.8084e-01  l1 = -0.019261  l2 = 0.002576\n",
            "tf_epoch =     80  elapsed = 00:51  loss = 4.0894e-01  error = 8.7435e-01  l1 = -0.009836  l2 = 0.002611\n",
            "tf_epoch =     90  elapsed = 00:57  loss = 3.7797e-01  error = 8.7022e-01  l1 = -0.006119  l2 = 0.002657\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 01:09  loss = 2.9271e-01  error = 8.6622e-01  l1 = -0.002416  l2 = 0.002700\n",
            "nt_epoch =     20  elapsed = 01:16  loss = 2.7790e-01  error = 8.6454e-01  l1 = 0.001315  l2 = 0.002696\n",
            "nt_epoch =     30  elapsed = 01:22  loss = 2.6894e-01  error = 8.6226e-01  l1 = 0.006638  l2 = 0.002688\n",
            "nt_epoch =     40  elapsed = 01:28  loss = 2.5875e-01  error = 8.6127e-01  l1 = 0.009240  l2 = 0.002682\n",
            "nt_epoch =     50  elapsed = 01:35  loss = 2.5109e-01  error = 8.6009e-01  l1 = 0.013117  l2 = 0.002667\n",
            "nt_epoch =     60  elapsed = 01:41  loss = 2.4493e-01  error = 8.6030e-01  l1 = 0.018070  l2 = 0.002613\n",
            "nt_epoch =     70  elapsed = 01:47  loss = 2.4260e-01  error = 8.6479e-01  l1 = 0.017433  l2 = 0.002530\n",
            "nt_epoch =     80  elapsed = 01:54  loss = 2.4071e-01  error = 8.7063e-01  l1 = 0.019109  l2 = 0.002396\n",
            "nt_epoch =     90  elapsed = 02:00  loss = 2.3877e-01  error = 8.8327e-01  l1 = 0.021897  l2 = 0.002116\n",
            "nt_epoch =    100  elapsed = 02:07  loss = 2.3790e-01  error = 8.9073e-01  l1 = 0.024235  l2 = 0.001943\n",
            "nt_epoch =    110  elapsed = 02:13  loss = 2.3706e-01  error = 9.0303e-01  l1 = 0.030477  l2 = 0.001635\n",
            "nt_epoch =    120  elapsed = 02:20  loss = 2.3665e-01  error = 9.0624e-01  l1 = 0.030470  l2 = 0.001571\n",
            "nt_epoch =    130  elapsed = 02:26  loss = 2.3619e-01  error = 9.1739e-01  l1 = 0.032333  l2 = 0.001329\n",
            "nt_epoch =    140  elapsed = 02:33  loss = 2.3493e-01  error = 9.4218e-01  l1 = 0.029549  l2 = 0.000861\n",
            "nt_epoch =    150  elapsed = 02:39  loss = 2.3421e-01  error = 9.5456e-01  l1 = 0.019215  l2 = 0.000717\n",
            "nt_epoch =    160  elapsed = 02:46  loss = 2.3361e-01  error = 9.6524e-01  l1 = 0.011744  l2 = 0.000578\n",
            "nt_epoch =    170  elapsed = 02:52  loss = 2.3307e-01  error = 9.7468e-01  l1 = 0.002941  l2 = 0.000477\n",
            "nt_epoch =    180  elapsed = 02:59  loss = 2.3223e-01  error = 9.8699e-01  l1 = -0.009289  l2 = 0.000353\n",
            "nt_epoch =    190  elapsed = 03:06  loss = 2.3153e-01  error = 9.9602e-01  l1 = -0.017565  l2 = 0.000255\n",
            "nt_epoch =    200  elapsed = 03:12  loss = 2.3027e-01  error = 1.0052e+00  l1 = -0.024447  l2 = 0.000140\n",
            "nt_epoch =    210  elapsed = 03:18  loss = 2.2923e-01  error = 1.0046e+00  l1 = -0.020375  l2 = 0.000112\n",
            "nt_epoch =    220  elapsed = 03:25  loss = 2.2813e-01  error = 1.0066e+00  l1 = -0.021208  l2 = 0.000079\n",
            "nt_epoch =    230  elapsed = 03:31  loss = 2.2713e-01  error = 1.0071e+00  l1 = -0.021031  l2 = 0.000069\n",
            "nt_epoch =    240  elapsed = 03:38  loss = 2.2600e-01  error = 1.0053e+00  l1 = -0.016111  l2 = 0.000055\n",
            "nt_epoch =    250  elapsed = 03:44  loss = 2.2520e-01  error = 1.0046e+00  l1 = -0.014192  l2 = 0.000050\n",
            "nt_epoch =    260  elapsed = 03:51  loss = 2.2451e-01  error = 1.0016e+00  l1 = -0.007848  l2 = 0.000046\n",
            "nt_epoch =    270  elapsed = 03:57  loss = 2.2331e-01  error = 1.0011e+00  l1 = -0.005989  l2 = 0.000039\n",
            "nt_epoch =    280  elapsed = 04:03  loss = 2.2250e-01  error = 1.0002e+00  l1 = -0.003369  l2 = 0.000030\n",
            "nt_epoch =    290  elapsed = 04:10  loss = 2.2198e-01  error = 9.9986e-01  l1 = -0.002819  l2 = 0.000031\n",
            "nt_epoch =    300  elapsed = 04:16  loss = 2.2093e-01  error = 9.9852e-01  l1 = 0.000250  l2 = 0.000027\n",
            "nt_epoch =    310  elapsed = 04:23  loss = 2.1993e-01  error = 9.9786e-01  l1 = 0.002516  l2 = 0.000018\n",
            "nt_epoch =    320  elapsed = 04:29  loss = 2.1933e-01  error = 9.9760e-01  l1 = 0.003167  l2 = 0.000016\n",
            "nt_epoch =    330  elapsed = 04:35  loss = 2.1827e-01  error = 9.9319e-01  l1 = 0.012067  l2 = 0.000016\n",
            "nt_epoch =    340  elapsed = 04:42  loss = 2.1775e-01  error = 9.9244e-01  l1 = 0.013583  l2 = 0.000015\n",
            "nt_epoch =    350  elapsed = 04:48  loss = 2.1709e-01  error = 9.9144e-01  l1 = 0.015692  l2 = 0.000014\n",
            "nt_epoch =    360  elapsed = 04:55  loss = 2.1668e-01  error = 9.9132e-01  l1 = 0.015985  l2 = 0.000014\n",
            "nt_epoch =    370  elapsed = 05:01  loss = 2.1613e-01  error = 9.8849e-01  l1 = 0.021836  l2 = 0.000012\n",
            "nt_epoch =    380  elapsed = 05:07  loss = 2.1536e-01  error = 9.8534e-01  l1 = 0.028348  l2 = 0.000010\n",
            "nt_epoch =    390  elapsed = 05:14  loss = 2.1511e-01  error = 9.8373e-01  l1 = 0.031691  l2 = 0.000009\n",
            "nt_epoch =    400  elapsed = 05:20  loss = 2.1369e-01  error = 9.8329e-01  l1 = 0.032758  l2 = 0.000007\n",
            "nt_epoch =    410  elapsed = 05:27  loss = 2.1297e-01  error = 9.8307e-01  l1 = 0.033306  l2 = 0.000005\n",
            "nt_epoch =    420  elapsed = 05:33  loss = 2.1134e-01  error = 9.8402e-01  l1 = 0.031537  l2 = 0.000004\n",
            "nt_epoch =    430  elapsed = 05:39  loss = 2.1031e-01  error = 9.8459e-01  l1 = 0.030398  l2 = 0.000004\n",
            "nt_epoch =    440  elapsed = 05:46  loss = 2.0908e-01  error = 9.8212e-01  l1 = 0.035499  l2 = 0.000003\n",
            "nt_epoch =    450  elapsed = 05:52  loss = 2.0777e-01  error = 9.8225e-01  l1 = 0.035329  l2 = 0.000002\n",
            "nt_epoch =    460  elapsed = 05:59  loss = 2.0683e-01  error = 9.8336e-01  l1 = 0.033109  l2 = 0.000002\n",
            "nt_epoch =    470  elapsed = 06:05  loss = 2.0578e-01  error = 9.8373e-01  l1 = 0.032413  l2 = 0.000001\n",
            "nt_epoch =    480  elapsed = 06:12  loss = 2.0488e-01  error = 9.8326e-01  l1 = 0.033371  l2 = 0.000001\n",
            "nt_epoch =    490  elapsed = 06:18  loss = 2.0373e-01  error = 9.8538e-01  l1 = 0.029153  l2 = 0.000001\n",
            "nt_epoch =    500  elapsed = 06:25  loss = 2.0293e-01  error = 9.8830e-01  l1 = 0.023339  l2 = 0.000001\n",
            "nt_epoch =    510  elapsed = 06:31  loss = 2.0213e-01  error = 9.8903e-01  l1 = 0.021884  l2 = 0.000001\n",
            "nt_epoch =    520  elapsed = 06:37  loss = 2.0151e-01  error = 9.9061e-01  l1 = 0.018713  l2 = 0.000001\n",
            "nt_epoch =    530  elapsed = 06:44  loss = 2.0033e-01  error = 9.9393e-01  l1 = 0.012102  l2 = 0.000000\n",
            "nt_epoch =    540  elapsed = 06:50  loss = 1.9962e-01  error = 9.9517e-01  l1 = 0.009629  l2 = 0.000000\n",
            "nt_epoch =    550  elapsed = 06:57  loss = 1.9858e-01  error = 9.9588e-01  l1 = 0.008211  l2 = 0.000000\n",
            "nt_epoch =    560  elapsed = 07:03  loss = 1.9770e-01  error = 9.9497e-01  l1 = 0.010033  l2 = 0.000000\n",
            "nt_epoch =    570  elapsed = 07:09  loss = 1.9650e-01  error = 9.9573e-01  l1 = 0.008523  l2 = 0.000000\n",
            "nt_epoch =    580  elapsed = 07:16  loss = 1.9543e-01  error = 9.9685e-01  l1 = 0.006271  l2 = 0.000000\n",
            "nt_epoch =    590  elapsed = 07:22  loss = 1.9438e-01  error = 9.9489e-01  l1 = 0.010204  l2 = 0.000000\n",
            "nt_epoch =    600  elapsed = 07:29  loss = 1.9362e-01  error = 9.9410e-01  l1 = 0.011783  l2 = 0.000000\n",
            "nt_epoch =    610  elapsed = 07:35  loss = 1.9276e-01  error = 9.9387e-01  l1 = 0.012251  l2 = 0.000000\n",
            "nt_epoch =    620  elapsed = 07:42  loss = 1.9190e-01  error = 9.9113e-01  l1 = 0.017721  l2 = 0.000000\n",
            "nt_epoch =    630  elapsed = 07:48  loss = 1.9124e-01  error = 9.8967e-01  l1 = 0.020655  l2 = 0.000000\n",
            "nt_epoch =    640  elapsed = 07:55  loss = 1.8992e-01  error = 9.8399e-01  l1 = 0.032012  l2 = 0.000000\n",
            "nt_epoch =    650  elapsed = 08:01  loss = 1.8932e-01  error = 9.8234e-01  l1 = 0.035309  l2 = 0.000000\n",
            "nt_epoch =    660  elapsed = 08:08  loss = 1.8837e-01  error = 9.7978e-01  l1 = 0.040437  l2 = 0.000000\n",
            "nt_epoch =    670  elapsed = 08:14  loss = 1.8770e-01  error = 9.7762e-01  l1 = 0.044748  l2 = 0.000000\n",
            "nt_epoch =    680  elapsed = 08:21  loss = 1.8682e-01  error = 9.7636e-01  l1 = 0.047277  l2 = 0.000000\n",
            "nt_epoch =    690  elapsed = 08:27  loss = 1.8605e-01  error = 9.7743e-01  l1 = 0.045144  l2 = 0.000000\n",
            "nt_epoch =    700  elapsed = 08:34  loss = 1.8489e-01  error = 9.7770e-01  l1 = 0.044592  l2 = 0.000000\n",
            "nt_epoch =    710  elapsed = 08:40  loss = 1.8432e-01  error = 9.7776e-01  l1 = 0.044479  l2 = 0.000000\n",
            "nt_epoch =    720  elapsed = 08:47  loss = 1.8332e-01  error = 9.7847e-01  l1 = 0.043049  l2 = 0.000000\n",
            "nt_epoch =    730  elapsed = 08:53  loss = 1.8237e-01  error = 9.7852e-01  l1 = 0.042953  l2 = 0.000000\n",
            "nt_epoch =    740  elapsed = 08:59  loss = 1.8165e-01  error = 9.7835e-01  l1 = 0.043302  l2 = 0.000000\n",
            "nt_epoch =    750  elapsed = 09:06  loss = 1.8090e-01  error = 9.7891e-01  l1 = 0.042184  l2 = 0.000000\n",
            "nt_epoch =    760  elapsed = 09:12  loss = 1.8020e-01  error = 9.8064e-01  l1 = 0.038724  l2 = 0.000000\n",
            "nt_epoch =    770  elapsed = 09:19  loss = 1.8154e-01  error = 9.8194e-01  l1 = 0.036120  l2 = 0.000000\n",
            "nt_epoch =    780  elapsed = 09:25  loss = 1.7824e-01  error = 9.8315e-01  l1 = 0.033695  l2 = 0.000000\n",
            "nt_epoch =    790  elapsed = 09:31  loss = 1.7748e-01  error = 9.8560e-01  l1 = 0.028796  l2 = 0.000000\n",
            "nt_epoch =    800  elapsed = 09:38  loss = 1.7676e-01  error = 9.8578e-01  l1 = 0.028449  l2 = 0.000000\n",
            "nt_epoch =    810  elapsed = 09:44  loss = 1.7635e-01  error = 9.8491e-01  l1 = 0.030182  l2 = 0.000000\n",
            "nt_epoch =    820  elapsed = 09:51  loss = 1.7562e-01  error = 9.8393e-01  l1 = 0.032137  l2 = 0.000000\n",
            "nt_epoch =    830  elapsed = 09:57  loss = 1.7533e-01  error = 9.8252e-01  l1 = 0.034956  l2 = 0.000000\n",
            "nt_epoch =    840  elapsed = 10:04  loss = 1.7486e-01  error = 9.8043e-01  l1 = 0.039142  l2 = 0.000000\n",
            "nt_epoch =    850  elapsed = 10:10  loss = 1.7436e-01  error = 9.7974e-01  l1 = 0.040513  l2 = 0.000000\n",
            "nt_epoch =    860  elapsed = 10:17  loss = 1.7393e-01  error = 9.7874e-01  l1 = 0.042522  l2 = 0.000000\n",
            "nt_epoch =    870  elapsed = 10:23  loss = 1.7327e-01  error = 9.7778e-01  l1 = 0.044449  l2 = 0.000000\n",
            "nt_epoch =    880  elapsed = 10:30  loss = 1.7274e-01  error = 9.7613e-01  l1 = 0.047747  l2 = 0.000000\n",
            "nt_epoch =    890  elapsed = 10:36  loss = 1.7220e-01  error = 9.7489e-01  l1 = 0.050212  l2 = 0.000000\n",
            "nt_epoch =    900  elapsed = 10:43  loss = 1.7166e-01  error = 9.7525e-01  l1 = 0.049494  l2 = 0.000000\n",
            "nt_epoch =    910  elapsed = 10:49  loss = 1.7120e-01  error = 9.7553e-01  l1 = 0.048933  l2 = 0.000000\n",
            "nt_epoch =    920  elapsed = 10:55  loss = 1.7093e-01  error = 9.7655e-01  l1 = 0.046907  l2 = 0.000000\n",
            "nt_epoch =    930  elapsed = 11:02  loss = 1.7066e-01  error = 9.7741e-01  l1 = 0.045179  l2 = 0.000000\n",
            "nt_epoch =    940  elapsed = 11:08  loss = 1.7036e-01  error = 9.7832e-01  l1 = 0.043365  l2 = 0.000000\n",
            "nt_epoch =    950  elapsed = 11:15  loss = 1.7012e-01  error = 9.7868e-01  l1 = 0.042645  l2 = 0.000000\n",
            "nt_epoch =    960  elapsed = 11:21  loss = 1.6963e-01  error = 9.8121e-01  l1 = 0.037582  l2 = 0.000000\n",
            "nt_epoch =    970  elapsed = 11:27  loss = 1.6928e-01  error = 9.8240e-01  l1 = 0.035202  l2 = 0.000000\n",
            "nt_epoch =    980  elapsed = 11:34  loss = 1.6887e-01  error = 9.8482e-01  l1 = 0.030357  l2 = 0.000000\n",
            "nt_epoch =    990  elapsed = 11:40  loss = 1.6848e-01  error = 9.8692e-01  l1 = 0.026160  l2 = 0.000000\n",
            "==================\n",
            "Training finished (epoch 100): duration = 11:46  error = 9.8855e-01  l1 = 0.022905  l2 = 0.000000\n",
            "l1:  0.022904953\n",
            "l2:  1.4382778e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBCNYg11Nq8W",
        "outputId": "53fd778d-5f7f-4014-8ceb-b49a53d70ae8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 20, 20, 20, 20, 20, 20, 20, 20, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ED89Xt6nR3nl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}