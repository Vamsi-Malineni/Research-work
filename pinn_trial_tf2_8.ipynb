{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pinn_trial_tf2.8",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOI9om2fG0yd2snjmfW6P63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WOp0QV73geKH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp \n",
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import time \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pinn(object):\n",
        "  def __init__(self,layers,optimizer,ub,lb):\n",
        "    \n",
        "    self.model=tf.keras.Sequential()\n",
        "    \n",
        "    self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    \n",
        "    self.model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X-lb)/(ub-lb)-1))\n",
        "\n",
        "    for width in layers[1:]:\n",
        "      self.model.add(tf.keras.layers.Dense(\n",
        "          width,activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'\n",
        "      ))\n",
        "\n",
        "  # finding the sizes of weights and biases for each layer in the neural network  \n",
        "    self.sizes_w=[]\n",
        "    self.sizes_b=[]\n",
        "\n",
        "    for i,width in enumerate(layers):\n",
        "      if i!=1:\n",
        "        self.sizes_w.append(int(width*layers[1]))\n",
        "        self.sizes_b.append(int(width if i!=0 else layers[1]))\n",
        "    self.dtype=tf.float32\n",
        "\n",
        "  # Defining two variables that have to be learnt (lambda1,lambda2)\n",
        "    self.lambda1=tf.Variable([0.0],dtype=self.dtype)\n",
        "    self.lambda2=tf.Variable([0.0],dtype=self.dtype)\n",
        "\n",
        "    self.optimizer=optimizer\n",
        "\n",
        "  def get_parameters(self,numpy=False):\n",
        "    l1=self.lambda1\n",
        "    l2=self.lambda2\n",
        "    if numpy :\n",
        "      return l1.numpy()[0],l2.numpy()[0]\n",
        "    return l1,l2\n",
        "\n",
        "  def setup_pinn(self,xtrain,ytrain,ttrain):\n",
        "    l1,l2=self.get_parameters()\n",
        "    # Inputs to be watched by gradient tape\n",
        "    x=tf.convert_to_tensor(xtrain,dtype=self.dtype)\n",
        "    y=tf.convert_to_tensor(ytrain,dtype=self.dtype)\n",
        "    t=tf.convert_to_tensor(ttrain,dtype=self.dtype)\n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      g.watch(x)\n",
        "      g.watch(y)\n",
        "      g.watch(t)\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "xpmFYlk1Llzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}