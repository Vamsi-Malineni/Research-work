{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pinn_trial_tf2.8",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb",
      "authorship_tag": "ABX9TyMcKCJQg5BLuHC/HwRyRKPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WOp0QV73geKH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp \n",
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import time \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "  def __init__(self,frequency=10):\n",
        "    self.start_time =time.time()\n",
        "    self.frequency=frequency\n",
        "\n",
        "  def elapsed_time(self):\n",
        "    return datetime.fromtimestamp(time.time()-self.start_time).strftime(\"%M:%S\")\n",
        "  \n",
        "  def get_error(self):\n",
        "    return self.error_func()\n",
        "  \n",
        "  def set_error(self,error_func):\n",
        "    self.error_func=error_func\n",
        "\n",
        "  def log_train_start(self,model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    self.model = model\n",
        "    print(self.model.summary()) \n",
        "  \n",
        "  def log_train_epoch(self,epoch,loss,custom=\"\",is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.elapsed_time()}  loss = {loss:.4e}  error = {self.get_error():.4e}  \" + custom)\n",
        "\n",
        "  def log_train_opti(self,name):\n",
        "    print(f\"---Startting {name} optimization function---\")\n",
        "  \n",
        "  def log_train_end(self,epoch,custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.elapsed_time()}  error = {self.get_error():.4e}  \" + custom)"
      ],
      "metadata": {
        "id": "6rH9bggGSiqn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LBFGS Optimizer"
      ],
      "metadata": {
        "id": "q5GHfjUbzzYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credits: Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "metadata": {
        "id": "bBNx4DFazGX3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer setup"
      ],
      "metadata": {
        "id": "Od6mJQl-1DMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up ADAM optimizer\n",
        "adam_epochs=100\n",
        "adam_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "# Setting up LBFGS optimizer\n",
        "lbfgs_epochs=1000\n",
        "lbfgs_config=Struct()\n",
        "lbfgs_config.learningRate=0.8\n",
        "lbfgs_config.maxIter=lbfgs_epochs\n",
        "lbfgs_config.nCorrection=50\n",
        "lbfgs_config.tolFun=1.0*np.finfo(float).eps"
      ],
      "metadata": {
        "id": "jFYefF85yalJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PINN class implementation"
      ],
      "metadata": {
        "id": "H1dO0Dot1GCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pinn:\n",
        "  def __init__(self,layers,optimizer,logger,ub,lb):\n",
        "    # This model takes x,y,t as inputs hence 3 inputs\n",
        "    # This model gives psi and p as outputs hence 2 outputs\n",
        "    # This model is of no use for us, as we take its derivatie and get \n",
        "    # u and v which are used for comparing with the ground truth\n",
        "    self.model=tf.keras.Sequential()\n",
        "    \n",
        "    self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    \n",
        "    # Here X should be tf.concat([x_train,y_train,t_train],1) such that scaling \n",
        "    # happens properly\n",
        "    self.model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X-lb)/(ub-lb)-1))\n",
        "    \n",
        "    # Initializing the hidden layers\n",
        "    for width in layers[1:]:\n",
        "      self.model.add(tf.keras.layers.Dense(\n",
        "          width,activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'\n",
        "      ))\n",
        "\n",
        "  # finding the sizes of weights and biases for each layer in the neural network  \n",
        "    self.sizes_w=[]\n",
        "    self.sizes_b=[]\n",
        "\n",
        "    for i,width in enumerate(layers):\n",
        "      if i!=1:\n",
        "        self.sizes_w.append(int(width*layers[1]))\n",
        "        self.sizes_b.append(int(width if i!=0 else layers[1]))\n",
        "    \n",
        "    self.dtype=tf.float32\n",
        "\n",
        "  # Defining two variables that have to be learnt (lambda1,lambda2)\n",
        "    self.lambda1=tf.Variable([0.0],dtype=self.dtype)\n",
        "    self.lambda2=tf.Variable([0.0],dtype=self.dtype)\n",
        "\n",
        "  # Setting the optimizer as adam optimizer  \n",
        "    self.optimizer=optimizer\n",
        "\n",
        "  def get_parameters(self,numpy=False):\n",
        "    l1=self.lambda1\n",
        "    l2=self.lambda2\n",
        "    if numpy :\n",
        "      return l1.numpy()[0],l2.numpy()[0]\n",
        "    return l1,l2\n",
        "\n",
        "  # f_g_model gives the required outputs in the form of u and v, \n",
        "  # takes in x,y,and t as inputs and passes them to (model), this function then\n",
        "  # differentiates and gives us the req. outputs\n",
        "  def f_g_model(self,xtrain,ytrain,ttrain):\n",
        "    l1,l2=self.get_parameters()\n",
        "    # Inputs to be watched by gradient tape\n",
        "    x_f=tf.convert_to_tensor(xtrain,dtype=self.dtype)\n",
        "    y_f=tf.convert_to_tensor(ytrain,dtype=self.dtype)\n",
        "    t_f=tf.convert_to_tensor(ttrain,dtype=self.dtype)\n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      g.watch(x_f)\n",
        "      g.watch(y_f)\n",
        "      g.watch(t_f)\n",
        "      # Stacking the input variables and sending them to model\n",
        "      xf=tf.stack([x_f[:,0],y_f[:,0],t_f[:,0]],axis=1)\n",
        "\n",
        "      psi_and_p=self.model(xf)\n",
        "\n",
        "      psi=psi_and_p[:,0:1]\n",
        "      p=psi_and_p[:,1:2]\n",
        "\n",
        "      u= g.gradient(psi,y_f)\n",
        "      v=-g.gradient(psi,x_f)\n",
        "\n",
        "      ut=g.gradient(u,t_f)\n",
        "      ux=g.gradient(u,x_f)\n",
        "      uy=g.gradient(u,y_f)\n",
        "      \n",
        "      vt=g.gradient(v,t_f)\n",
        "      vx=g.gradient(v,x_f)\n",
        "      vy=g.gradient(v,y_f)\n",
        "\n",
        "      px=g.gradient(p,x_f)\n",
        "      py=g.gradient(p,y_f)\n",
        "    \n",
        "    uxx=g.gradient(ux,x_f)\n",
        "    uyy=g.gradient(uy,y_f)\n",
        "    vxx=g.gradient(vx,x_f)\n",
        "    vyy=g.gradient(vy,y_f)\n",
        "\n",
        "    del g\n",
        "    f = ut+l1*(u*ux+v*uy)+px-l2*(uxx+uyy) \n",
        "    g = vt+l1*(u*vx+v*vy)+py-l2*(vxx+vyy)\n",
        "    \n",
        "    return u,v,p,f,g\n",
        "    \n",
        "  def loss(self,x,y,t,u,v):\n",
        "      u_pred,v_pred,p_pred,f_pred,g_pred=self.f_g_model(x,y,t)\n",
        "      \n",
        "      loss_eq=tf.reduce_mean(tf.square(u-u_pred))+ \\\n",
        "              tf.reduce_mean(tf.square(v-v_pred))+ \\\n",
        "              tf.reduce_mean(tf.square(f_pred))+ \\\n",
        "              tf.reduce_mean(tf.square(g_pred))\n",
        "      return loss_eq\n",
        "    \n",
        "  def training_variables(self):\n",
        "      variables=self.model.trainable_variables\n",
        "      variables.extend([self.lambda1,self.lambda2])\n",
        "      return variables\n",
        "\n",
        "  def grad(self,x,y,t,u,v):\n",
        "      with tf.GradientTape() as g:\n",
        "        loss_value=self.loss(x,y,t,u,v)\n",
        "      return loss_value,g.gradient(loss_value,self.training_variables())\n",
        "    \n",
        "    # Getting weights for LBFGS optimizer for the start point\n",
        "  def get_weights(self):\n",
        "      weights_and_biases=[]\n",
        "      \n",
        "      for layer in self.model.layers[1:]:\n",
        "        wandb=layer.get_weights()\n",
        "        w=wandb[0].flatten()\n",
        "        b=wandb[1]\n",
        "        weights_and_biases.extend(w)\n",
        "        weights_and_biases.extend(b)\n",
        "      weights_and_biases(self.lambda1.numpy())\n",
        "      weights_and_biases(self.lambda2.numpy())\n",
        "\n",
        "      return tf.convert_to_tensor(weights_and_biases,dtype=self.dtype)\n",
        "    \n",
        "    # Setting up the weights\n",
        "  def set_weights(self,w):\n",
        "      for j,layer in enumerate(self.model.layers[1:]):\n",
        "        start=sum(self.sizes_w[:j])+sum(self.sizes_b[:j])\n",
        "        ending=sum(self.sizes_w[:j+1])+sum(self.sizes_b[:j])\n",
        "        weights=w[start:ending]\n",
        "        w_div=int(self.sizes_w[j]/self.sizes_b[j])\n",
        "        weights=tf.reshape(weights,[w_div,self.sizes_b[j]])\n",
        "        biases=w[ending:ending+self.sizes_b[i]]\n",
        "        weightsandbiases=[weights,biases]\n",
        "        layer.set_weights(weightsandbiases)\n",
        "      self.lambda1.assign([w[-2]])\n",
        "      self.lambda2.assign([w[-1]])\n",
        "\n",
        "    # Getting the lambda1 and lambda2 parameters after training\n",
        "  def get_lambdas(self,numpy=False):\n",
        "      l1=self.lambda1\n",
        "      l2=self.lambda2\n",
        "      if numpy:\n",
        "        return l1.numpy()[0],l2.numpy()[0]\n",
        "      return l1,l2\n",
        "\n",
        "    # Training function \n",
        "  def train(self,x,y,t,u,v,epochs,config):\n",
        "      x=tf.convert_to_tensor(x,dtype=self.dtype)\n",
        "      y=tf.convert_to_tensor(y,dtype=self.dtype)\n",
        "      t=tf.convert_to_tensor(t,dtype=self.dtype)\n",
        "      u=tf.convert_to_tensor(u,dtype=self.dtype)\n",
        "      v=tf.convert_to_tensor(v,dtype=self.dtype)\n",
        "      \n",
        "      def log_train_epoch(epoch,loss,is_iter):\n",
        "        l1,l2=self.get_lambdas(numpy=True)\n",
        "        custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
        "        self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
        "      \n",
        "      # applying adam optimzer\n",
        "      self.logger.log_train_opti(\"Adam\")\n",
        "\n",
        "      loss_value,grads=self.grad(x,y,t,u,v)\n",
        "      \n",
        "      for epoch in range(adam_epochs):\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(grads,self.training_variables()))\n",
        "        log_train_epoch(epoch,loss_value,False)\n",
        "        \n",
        "      # applying lbfgs optimizer\n",
        "      self.logger.log_train_opti(\"LBFGS\")\n",
        "\n",
        "      def loss_and_flat_grad(w):\n",
        "        with tf.GradientTape() as tape:\n",
        "          self.set_weights(w)\n",
        "          tape.watch(self.lambda1)\n",
        "          tape.watch(self.lambda2)\n",
        "          loss_values=self.loss(x,y,t,u,v)\n",
        "        grad=tape.gradient(loss_values,self.training_variables())\n",
        "        grad_flat=[]\n",
        "        for g in grad:\n",
        "          grad_flat.append(tf.reshape(g,[-1]))\n",
        "        grad_flat=tf.concat(grad_flat,0)\n",
        "\n",
        "        return loss_values,grad_flat\n",
        "      \n",
        "      lbfgs(loss_and_flat_grad,self.get_weights(),lbfgs_config,Struct(),True,log_train_epoch)\n",
        "      l1,l2=self.get_lambdas(numpy=True)\n",
        "      self.logger.log_train_end(lbfgs_epoch,f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xpmFYlk1Llzx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepdata(N_train):\n",
        "    #data = scipy.io.loadmat('Downloads/cylinder_nektar_wake.mat')\n",
        "    data=scipy.io.loadmat('/content/drive/MyDrive/cylinder_nektar_wake.mat')\n",
        "    U_star = data['U_star'] # N x 2 x T\n",
        "    t_star = data['t'] # T x 1\n",
        "    X_star = data['X_star'] # N x 2\n",
        "    \n",
        "    N = X_star.shape[0]\n",
        "    T = t_star.shape[0]\n",
        "    \n",
        "    # Rearrange Data \n",
        "    XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "    YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "    TT = np.tile(t_star, (1,N)).T # N x T\n",
        "    \n",
        "    UU = U_star[:,0,:] # N x T\n",
        "    VV = U_star[:,1,:] # N x T\n",
        "    \n",
        "    x = XX.flatten()[:,None] # NT x 1\n",
        "    y = YY.flatten()[:,None] # NT x 1\n",
        "    t = TT.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    u = UU.flatten()[:,None] # NT x 1\n",
        "    v = VV.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    # Training Data    \n",
        "    idx = np.random.choice(N*T, N_train, replace=False)\n",
        "    x_train = x[idx,:]\n",
        "    y_train = y[idx,:]\n",
        "    t_train = t[idx,:]\n",
        "    u_train = u[idx,:]\n",
        "    v_train = v[idx,:]\n",
        "    \n",
        "    lb=tf.math.reduce_min(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0) \n",
        "    ub=tf.math.reduce_max(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0)\n",
        "    lb=tf.cast(lb,dtype=tf.float32)\n",
        "    ub=tf.cast(ub,dtype=tf.float32)\n",
        "\n",
        "    return x_train,y_train,t_train,u_train,v_train,lb,ub\n"
      ],
      "metadata": {
        "id": "KTtmMeF2P19e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training "
      ],
      "metadata": {
        "id": "56vMQrBSeN4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,t,u,v,lb,ub=prepdata(2000)\n",
        "layers=[3,20,20,20,20,20,20,20,20,2]\n",
        "lambdas_star=[1,0.01]\n",
        "\n",
        "logger=Logger(frequency=10)\n",
        "phyinn=pinn(layers,adam_optimizer,logger,ub,lb)\n",
        "\n",
        "def error():\n",
        "  l1,l2=phyinn.get_lambdas(numpy=True)\n",
        "  l1_star=lambdas_star[0]\n",
        "  l2_star=lambdas_star[1]\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "\n",
        "logger.set_error(error)\n",
        "\n",
        "phyinn.train(x,y,t,u,v,adam_epochs,lbfgs_config)\n",
        "\n",
        "l1_pred,l2_pred=phyinn.get_lambdas(numpy=True)\n",
        "\n",
        "print(\"l1: \",l1_pred)\n",
        "print(\"l2: \",l2_pred)"
      ],
      "metadata": {
        "id": "d90Lp5y3eNbH",
        "outputId": "3e50fd18-da90-4901-af07-e6b1c98c3380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2936730b1d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mphyinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madam_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlbfgs_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0ml1_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphyinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-6e63e799c955>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, t, u, v, epochs, config)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0;31m# applying adam optimzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_train_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'pinn' object has no attribute 'logger'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WmGJY6mehuDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}