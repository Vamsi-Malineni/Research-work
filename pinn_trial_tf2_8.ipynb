{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pinn_trial_tf2.8",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTGzUXKqtqUNsCxg3/Uu4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsi-Malineni/Research-work/blob/master/pinn_trial_tf2_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOp0QV73geKH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp \n",
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import time \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pinn:\n",
        "  def __init__(self,layers,optimizer,ub,lb):\n",
        "    \n",
        "    self.model=tf.keras.Sequential()\n",
        "    \n",
        "    self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    \n",
        "    self.model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X-lb)/(ub-lb)-1))\n",
        "\n",
        "    for width in layers[1:]:\n",
        "      self.model.add(tf.keras.layers.Dense(\n",
        "          width,activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'\n",
        "      ))\n",
        "\n",
        "  # finding the sizes of weights and biases for each layer in the neural network  \n",
        "    self.sizes_w=[]\n",
        "    self.sizes_b=[]\n",
        "\n",
        "    for i,width in enumerate(layers):\n",
        "      if i!=1:\n",
        "        self.sizes_w.append(int(width*layers[1]))\n",
        "        self.sizes_b.append(int(width if i!=0 else layers[1]))\n",
        "    self.dtype=tf.float32\n",
        "\n",
        "  # Defining two variables that have to be learnt (lambda1,lambda2)\n",
        "    self.lambda1=tf.Variable([0.0],dtype=self.dtype)\n",
        "    self.lambda2=tf.Variable([0.0],dtype=self.dtype)\n",
        "\n",
        "    self.optimizer=optimizer\n",
        "\n",
        "  def get_parameters(self,numpy=False):\n",
        "    l1=self.lambda1\n",
        "    l2=self.lambda2\n",
        "    if numpy :\n",
        "      return l1.numpy()[0],l2.numpy()[0]\n",
        "    return l1,l2\n",
        "\n",
        "  def setup_pinn(self,xtrain,ytrain,ttrain):\n",
        "    l1,l2=self.get_parameters()\n",
        "    # Inputs to be watched by gradient tape\n",
        "    x=tf.convert_to_tensor(xtrain,dtype=self.dtype)\n",
        "    y=tf.convert_to_tensor(ytrain,dtype=self.dtype)\n",
        "    t=tf.convert_to_tensor(ttrain,dtype=self.dtype)\n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      g.watch(x)\n",
        "      g.watch(y)\n",
        "      g.watch(t)\n",
        "      \n",
        "      "
      ],
      "metadata": {
        "id": "xpmFYlk1Llzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepdata(N_train):\n",
        "  data = scipy.io.loadmat('/content/drive/MyDrive/cylinder_nektar_wake.mat')\n",
        "           \n",
        "    U_star = data['U_star'] # N x 2 x T\n",
        "    t_star = data['t'] # T x 1\n",
        "    X_star = data['X_star'] # N x 2\n",
        "    \n",
        "    N = X_star.shape[0]\n",
        "    T = t_star.shape[0]\n",
        "    \n",
        "    # Rearrange Data \n",
        "    XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "    YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "    TT = np.tile(t_star, (1,N)).T # N x T\n",
        "    \n",
        "    UU = U_star[:,0,:] # N x T\n",
        "    VV = U_star[:,1,:] # N x T\n",
        "    \n",
        "    x = XX.flatten()[:,None] # NT x 1\n",
        "    y = YY.flatten()[:,None] # NT x 1\n",
        "    t = TT.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    u = UU.flatten()[:,None] # NT x 1\n",
        "    v = VV.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    # Training Data    \n",
        "    idx = np.random.choice(N*T, N_train, replace=False)\n",
        "    x_train = x[idx,:]\n",
        "    y_train = y[idx,:]\n",
        "    t_train = t[idx,:]\n",
        "    u_train = u[idx,:]\n",
        "    v_train = v[idx,:]\n",
        "    \n",
        "    lb=tf.math.reduce_min(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0) \n",
        "    ub=tf.math.reduce_max(tf.concat([x_train,y_train,t_train],1),keepdims=True,axis=0)\n",
        "        \n",
        "\n",
        "    return x_train,y_train,t_train,u_train,v_train,lb,ub\n"
      ],
      "metadata": {
        "id": "KTtmMeF2P19e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}